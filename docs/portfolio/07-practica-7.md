---
title: "Pr√°ctica 7"
date: 2025-09-16
---

# Pr√°ctica 7

## Contexto
En esta pr√°ctica n√∫mero 7 del curso, empezamos con los principios de Deep Learning.

## Objetivos
- Descubrir las limitaciones del perceptr√≥n simple (problema XOR).
- Resolver problemas reales con redes multicapa (sklearn MLP).
- Implementar redes neuronales profesionales (TensorFlow/PyTorch Lightning).
- Entender cu√°ndo usar cada herramienta.

## Actividades (con tiempos estimados)
- Parte 1 (40min)
- 

## Desarrollo


## Evidencias
- Se adjunta imagen "resultado-t7-parte1.png" en `docs/assets/`

## Reflexi√≥n

---

# Deep Learning: Redes Neuronales

## Setup inicial: C√≥digo

```python
import numpy as np
import matplotlib.pyplot as plt

# Funci√≥n perceptr√≥n b√°sica
def perceptron(x1, x2, w1, w2, bias):
    return 1 if (w1*x1 + w2*x2 + bias) >= 0 else 0

# Funci√≥n para visualizar el perceptr√≥n
def graficar_perceptron(w1, w2, bias, datos, resultados_esperados, titulo):
    plt.figure(figsize=(8, 6))

    # Graficar puntos
    for i, (x1, x2) in enumerate(datos):
        color = 'red' if resultados_esperados[i] == 0 else 'blue'
        marker = 'o' if resultados_esperados[i] == 0 else 's'
        plt.scatter(x1, x2, c=color, s=200, marker=marker, 
                   edgecolor='black', linewidth=2)
        plt.text(x1+0.05, x2+0.05, f'({x1},{x2})', fontsize=12)

    # Graficar l√≠nea de separaci√≥n: w1*x1 + w2*x2 + bias = 0
    if w2 != 0:  # Para evitar divisi√≥n por cero
        x_line = np.linspace(-0.5, 1.5, 100)
        y_line = -(w1*x_line + bias) / w2
        plt.plot(x_line, y_line, 'green', linewidth=3, alpha=0.8, 
                label=f'L√≠nea: {w1:.1f}x‚ÇÅ + {w2:.1f}x‚ÇÇ + {bias:.1f} = 0')

    plt.xlim(-0.3, 1.3)
    plt.ylim(-0.3, 1.3)
    plt.xlabel('x1', fontsize=14)
    plt.ylabel('x2', fontsize=14)
    plt.title(titulo, fontsize=16)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    print(f"üîç Interpretaci√≥n: Los puntos ROJOS (‚óã) son clase 0, los AZULES (‚ñ†) son clase 1")
    print(f"   La l√≠nea VERDE separa las clases. ¬øLo logra perfectamente?")
    print(f"üí° Record√°: Un perceptr√≥n es la ecuaci√≥n de una l√≠nea: y = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b")

# Datos para l√≥gica booleana
datos = np.array([[0,0], [0,1], [1,0], [1,1]])
print("üß† Vamos a entrenar un perceptr√≥n para l√≥gica booleana")
print("   (¬°Y ver c√≥mo funciona visualmente!)")
```

En este setup inicial, definimos una funci√≥n de perceptr√≥n simple y una funci√≥n para graficar los resultados. Tambi√©n preparamos los datos de entrada para las operaciones l√≥gicas AND, OR y XOR.

## Parte 1: Descripci√≥n
En esta primer parte de la pr√°ctica, resolveremos los argoritmos de l√≥gica booleana AND, OR y XOR usando un perceptr√≥n simple, para ellos usaremos la funci√≥n "perceptron" y "graficar_perceptron" definidas en el setup inicial.
Modificaremos los pesos y bias para cada caso.

## Parte 1: C√≥digo

```python
# === L√ìGICA AND ===
print("\n1Ô∏è‚É£ PROBLEMA AND: Solo verdadero cuando AMBAS entradas son 1")
print("x1 | x2 | AND esperado")
print(" 0 |  0 |      0")
print(" 0 |  1 |      0")
print(" 1 |  0 |      0") 
print(" 1 |  1 |      1")  # estudiantes completan

# Encontremos pesos que funcionen para AND
w1, w2, bias = 0.5, 0.5, 0.7  # pesos iguales, ¬øqu√© bias?

print(f"\nProbando AND con pesos: w1={w1}, w2={w2}, bias={bias}")
resultados_and = [0, 0, 0, 1]

for i, (x1, x2) in enumerate(datos):
    prediccion = perceptron(x1, x2, w1, w2, bias)
    esperado = resultados_and[i]
    ok = "‚úÖ" if prediccion == esperado else "‚ùå"
    print(f"  {x1},{x2} ‚Üí {prediccion} (esperado {esperado}) {ok}")

# üìä VISUALIZACI√ìN AND
graficar_perceptron(w1, w2, bias, datos, resultados_and, "Perceptr√≥n AND")
```
#### Resultados: soluci√≥n AND
<!-- ![Tabla comparativa](../assets/resultado-t7-parte1.1.png) -->



```python
# === L√ìGICA OR ===
print("\n2Ô∏è‚É£ PROBLEMA OR: Verdadero cuando AL MENOS UNA entrada es 1")
print("x1 | x2 | OR esperado")
print(" 0 |  0 |      0")
print(" 0 |  1 |      1")
print(" 1 |  0 |      1")
print(" 1 |  1 |      1")

# Para OR necesitamos ser m√°s permisivos
w1, w2, bias = 0.5, 0.5, 0.2  # ¬øqu√© bias permite que una sola entrada active?

print(f"\nProbando OR con pesos: w1={w1}, w2={w2}, bias={bias}")
resultados_or = [0, 1, 1, 1]

for i, (x1, x2) in enumerate(datos):
    prediccion = perceptron(x1, x2, w1, w2, bias)
    esperado = resultados_or[i]
    ok = "‚úÖ" if prediccion == esperado else "‚ùå"
    print(f"  {x1},{x2} ‚Üí {prediccion} (esperado {esperado}) {ok}")

# üìä VISUALIZACI√ìN OR
graficar_perceptron(w1, w2, bias, datos, resultados_or, "Perceptr√≥n OR")
```
#### Resultados: soluci√≥n OR
<!-- ![Tabla comparativa](../assets/resultado-t7-parte1.1.png) -->


