---
title: "PrÃ¡ctica 6"
date: 2025-09-09
---

# PrÃ¡ctica 6

## Contexto
En esta prÃ¡ctica nÃºmero 6 del curso tenemos la siguiente problemÃ¡tica: Los centros comerciales buscan conocer mejor a sus clientes, pero el problema a resolver es la segmentaciÃ³n de clientes, es decir, agruparlos segÃºn sus caracterÃ­sticas y comportamientos para poder diseÃ±ar campaÃ±as de marketing mÃ¡s personalizadas, ofrecer promociones especÃ­ficas, invertir mejor en publicidad y comprender cÃ³mo compran los distintos tipos de clientes.

## Objetivos
- Identificar 3-5 segmentos de clientes distintos usando K-Means.
- Aplicar tÃ©cnicas de normalizaciÃ³n (MinMax, Standard, Robust).
- Usar PCA para reducciÃ³n de dimensionalidad y visualizaciÃ³n.
- Comparar PCA con mÃ©todos de selecciÃ³n de features.
- Interpretar resultados desde perspectiva de negocio.

## Actividades (con tiempos estimados)
- Parte 1 (180min)
- Parte 2 (150min)
- Parte 3 (30min)
- Parte 4 (15min)
- Challenge 1 (50min)
- Challenge 2 (20min)
- Challenge 3 (60min)
- Challenge 4 ()
- Challenge 5 ()
- DiseÃ±o del github pages (200min)

## Desarrollo
En esta prÃ¡ctica trabajamos en segmentar clientes usando clustering y PCA. Primero exploramos y entendimos el dataset, viendo quÃ© variables eran mÃ¡s relevantes y codificando las categÃ³ricas. DespuÃ©s aplicamos distintos escaladores y PCA para reducir dimensiones, comparando con Feature Selection, y vimos que PCA daba mejores resultados. Con K-Means, y usando Elbow Method y Silhouette Score, identificamos 4 clusters que tenÃ­an sentido para el negocio, diferenciando perfiles como jÃ³venes gastadores y clientes mayores mÃ¡s conservadores.

## Evidencias
- Se adjunta imagen "resultado-t6-parte1.1.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.3.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.4.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.5.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.6.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.7.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.8.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.9.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte1.10.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte2.1.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte2.2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte2.3.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte2.4.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte2.5.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte2.6.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte2.7.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte3.1.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte3.2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte3.3.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte3.4.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte4.1.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte4.2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte5.1.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte5.2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte5.3.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte5.4.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-parte5.5.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto1.1.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto1.2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto1.3.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto1.4.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto3.1.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto3.2.png" en `docs/assets/`
- Se adjunta imagen "resultado-t6-reto3.3.png" en `docs/assets/`

## ReflexiÃ³n
Lo mÃ¡s desafiante fue preparar los datos sin perder informaciÃ³n relevante, pero la prÃ¡ctica mostrÃ³ la importancia de combinar comprensiÃ³n del negocio con tÃ©cnicas de anÃ¡lisis. Los clusters permiten pensar en estrategias de marketing mÃ¡s personalizadas, aunque hay limitaciones por el tamaÃ±o y simplificaciÃ³n del dataset. En general, se reforzÃ³ cÃ³mo la preparaciÃ³n de datos y la elecciÃ³n correcta de mÃ©todos impactan directamente en la calidad y utilidad de los resultados.

---

# Machine Learning ClÃ¡sico: Clustering y PCA - Mall Customer Segmentation

## Setup inicial: CÃ³digo

```python
# === IMPORTS BÃSICOS PARA EMPEZAR ===
import pandas as pd
import numpy as np

print("Iniciando anÃ¡lisis de Mall Customer Segmentation Dataset")
print("Pandas y NumPy cargados - listos para trabajar con datos")
```

## Parte 1: DescripciÃ³n
AquÃ­ empezamos por cargar el dataset de clientes, analizaremos sus atributos, los tipos de datos que manejamos y mÃ¡s.

## Parte 1: CÃ³digo

```python
# Descargar desde GitHub (opciÃ³n mÃ¡s confiable)
url = "https://raw.githubusercontent.com/SteffiPeTaffy/machineLearningAZ/master/Machine%20Learning%20A-Z%20Template%20Folder/Part%204%20-%20Clustering/Section%2024%20-%20K-Means%20Clustering/Mall_Customers.csv"

df_customers = pd.read_csv(url)

print("INFORMACIÃ“N DEL DATASET:")
print(f"Shape: {df_customers.shape[0]} filas, {df_customers.shape[1]} columnas")
print(f"Columnas: {list(df_customers.columns)}")
print(f"Memoria: {df_customers.memory_usage(deep=True).sum() / 1024:.1f} KB")

print(f"\nPRIMERAS 5 FILAS:")
df_customers.head()

# === ANÃLISIS DE TIPOS Y ESTRUCTURA ===
print("INFORMACIÃ“N DETALLADA DE COLUMNAS:")
print(df_customers.info())

print(f"\nESTADÃSTICAS DESCRIPTIVAS:")
df_customers.describe()

# === ANÃLISIS DE GÃ‰NERO ===
print("DISTRIBUCIÃ“N POR GÃ‰NERO:")
gender_counts = df_customers['Genre'].value_counts()
print(gender_counts)
print(f"\nPorcentajes:")
for gender, count in gender_counts.items():
    pct = (count / len(df_customers) * 100)
    print(f"   {gender}: {pct:.1f}%")

# === ESTADÃSTICAS DE VARIABLES DE SEGMENTACIÃ“N ===
numeric_vars = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']

print("ESTADÃSTICAS CLAVE:")
print(df_customers[numeric_vars].describe().round(2))

print(f"\nRANGOS OBSERVADOS:")
for var in numeric_vars:
    min_val, max_val = df_customers[var].min(), df_customers[var].max()
    mean_val = df_customers[var].mean()
    print(f"   {var}: {min_val:.0f} - {max_val:.0f} (promedio: {mean_val:.1f})")

# === DETECCIÃ“N DE OUTLIERS USANDO IQR ===
print("DETECCIÃ“N DE OUTLIERS:")

outlier_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']

for col in outlier_cols:
    Q1 = df_customers[col].quantile(0.25)
    Q3 = df_customers[col].quantile(0.75)
    IQR = Q3 - Q1

    # Calcular lÃ­mites
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Encontrar outliers
    outliers = df_customers[(df_customers[col] < lower_bound) | 
                           (df_customers[col] > upper_bound)]

    print(f"   {col}: {len(outliers)} outliers ({len(outliers)/len(df_customers)*100:.1f}%)")
    print(f"      LÃ­mites normales: {lower_bound:.1f} - {upper_bound:.1f}")

```
#### Resultados: info del dataset
![Tabla comparativa](../assets/resultado-t6-parte1.1.png)

Cargamos el dataset usando la funcion read.csv() de pandas y vemos informaciÃ³n general de las columnas que manipularemos, cantidad de columnas, cantidad de filas, memoria, tipos de datos y memoria usada por el dataset.

#### Resultados: anÃ¡lisis de datos
![Tabla comparativa](../assets/resultado-t6-parte1.2.png)

Aqui analizaremos estadÃ­sticas claves del dataset, como la cantidad de mujeres y hombres, vemos mÃ©tricas claves en los atributos numÃ©ricos del dataset, por ejemplo, la media de edad, de ingresos, el minimo y mÃ¡ximo de edad, y tambiÃ©n tenemos los valores de estos atributos para los quartiles, Q1, Q2 y Q3, representando el 25%, 50% y 75% de la informaciÃ³n respectivamente. Posteriormente, observamos los rangos que manejamos en nuestros datos, en este caso la edad, el ingreso anual y el puntaje de comprador; para lograr esto usamos las funciones de min(), max() y mean(), de manera de obtener el mÃ­nimo, mÃ¡ximo y promedio, facilitando el cÃ¡lculo.
Finalmente utilizamos los quartiles que nombramos para calcular outlines de los atributos, es decir, vemos el lÃ­mite inferior y superior en el que se varia esa mÃ©trica.

```python
# === IMPORTS PARA VISUALIZACIÃ“N ===
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar estilo
plt.style.use('default')
sns.set_palette("husl")

# === HISTOGRAMAS DE VARIABLES PRINCIPALES ===
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Distribuciones de Variables Clave', fontsize=14, fontweight='bold')

vars_to_plot = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']

for i, (var, color) in enumerate(zip(vars_to_plot, colors)):
    axes[i].hist(df_customers[var], bins=20, alpha=0.7, color=color, edgecolor='black')
    axes[i].set_title(f'{var}')
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Frecuencia')
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Resultados: visualizaciÃ³n de datos
![Tabla comparativa](../assets/resultado-t6-parte1.3.png)

Aqui observamos la frecuencia de los datos, para lograr esto, usamos la libreria matplotlib que nos permite graficar y la seaborn para agregarle formato a la visualizaciÃ³n.

```python
# === SCATTER PLOTS PARA RELACIONES CLAVE ===
fig, axes = plt.subplots(1, 3, figsize=(18, 5))
fig.suptitle('Relaciones Entre Variables', fontsize=14, fontweight='bold')

# Age vs Income
axes[0].scatter(df_customers['Age'], df_customers['Annual Income (k$)'], 
                alpha=0.6, color='#96CEB4', s=50)
axes[0].set_xlabel('Age (aÃ±os)')
axes[0].set_ylabel('Annual Income (k$)')
axes[0].set_title('Age vs Income')
axes[0].grid(True, alpha=0.3)

# Income vs Spending Score â­ CLAVE PARA SEGMENTACIÃ“N
axes[1].scatter(df_customers['Annual Income (k$)'], df_customers['Spending Score (1-100)'], 
                alpha=0.6, color='#FFEAA7', s=50)
axes[1].set_xlabel('Annual Income (k$)')
axes[1].set_ylabel('Spending Score (1-100)')
axes[1].set_title('Income vs Spending Score (CLAVE)')
axes[1].grid(True, alpha=0.3)

# Age vs Spending Score
axes[2].scatter(df_customers['Age'], df_customers['Spending Score (1-100)'], 
                alpha=0.6, color='#DDA0DD', s=50)
axes[2].set_xlabel('Age (aÃ±os)')
axes[2].set_ylabel('Spending Score (1-100)')
axes[2].set_title('Age vs Spending Score')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### Resultados: relaciÃ³n entre atributos
![Tabla comparativa](../assets/resultado-t6-parte1.4.png)

En esta visualizaciÃ³n buscamos observar la relaciÃ³n entre los atributos, por ejemplo, el puntaje de compra segÃºn la edad o el ingreso anual segun la edad/puntaje de compra.

```python
# === MATRIZ DE CORRELACIÃ“N ===
correlation_vars = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
corr_matrix = df_customers[correlation_vars].corr()

print("MATRIZ DE CORRELACIÃ“N:")
print(corr_matrix.round(3))

# Visualizar matriz de correlaciÃ³n
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, 
            fmt='.3f', linewidths=0.5, square=True)
plt.title('Matriz de CorrelaciÃ³n - Mall Customers')
plt.tight_layout()
plt.show()

print(f"\nCORRELACIÃ“N MÃS FUERTE:")
# Encontrar la correlaciÃ³n mÃ¡s alta (excluyendo diagonal)
corr_flat = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
max_corr = corr_flat.stack().idxmax()
max_val = corr_flat.stack().max()
print(f"   {max_corr[0]} â†” {max_corr[1]}: {max_val:.3f}")
```
#### Resultados: matriz de correlaciÃ³n
![Tabla comparativa](../assets/resultado-t6-parte1.6.png)

En esta matriz de correlaciÃ³n buscamos analizar entre atributos, cual son los que influyen mÃ¡s entre sÃ­. Por ejemplo, se observa que la edad no influye tanto en el puntaje de compra, si no que el ingreso anual es el que tiene mÃ¡s peso en ese aspecto.

```python
# === COMPARACIÃ“N ESTADÃSTICAS POR GÃ‰NERO ===
print("ANÃLISIS COMPARATIVO POR GÃ‰NERO:")

gender_stats = df_customers.groupby('Genre')[numeric_vars].agg(['mean', 'std']).round(2)
print(gender_stats)

print(f"\nINSIGHTS POR GÃ‰NERO:")
for var in numeric_vars:
    male_avg = df_customers[df_customers['Genre'] == 'Male'][var].mean()
    female_avg = df_customers[df_customers['Genre'] == 'Female'][var].mean()

    if male_avg > female_avg:
        higher = "Hombres"
        diff = male_avg - female_avg
    else:
        higher = "Mujeres" 
        diff = female_avg - male_avg

print(f"   {var}: {higher} tienen promedio mÃ¡s alto (diferencia: {diff:.1f})")
```
#### Resultados: anÃ¡lisis del gÃ©nero
![Tabla comparativa](../assets/resultado-t6-parte1.7.png)

Observamos un pequeÃ±o anÃ¡lisis de la influencia que tiene el gÃ©nero en el el puntaje de compra, y en este caso las mujeres tienen un promedio superior al hombre, es decir, son clientes mÃ¡s redituables econÃ³micamente. Utilizamos funciones como la media, mean() para el anÃ¡lisis.

```python
# === COMPLETE ESTOS INSIGHTS BASÃNDOTE EN LO OBSERVADO ===
print("INSIGHTS PRELIMINARES - COMPLETE:")

print(f"\nCOMPLETE BASÃNDOTE EN TUS OBSERVACIONES:")
print(f"   Variable con mayor variabilidad: Ingreso anual (Annual Income (k$))")
print(f"   Â¿Existe correlaciÃ³n fuerte entre alguna variable? Si, entre el ingreso anual y el puntaje de compra.")
print(f"   Â¿QuÃ© variable tiene mÃ¡s outliers? La edad.") 
print(f"   Â¿Los hombres y mujeres tienen patrones diferentes? Si, las mujeres tienen tendencia a tener mayor puntaje de compra, es decir, comprar mÃ¡s.")
print(f"   Â¿QuÃ© insight es mÃ¡s relevante para el anÃ¡lisis? La relaciÃ³n entre ingresos y puntaje de compra, porque define segmentos de clientes valiosos.")
print(f"   Â¿QuÃ© 2 variables serÃ¡n mÃ¡s importantes para clustering? Annual Income (k$) y Spending Score (1-100).")

print(f"\nPREPARÃNDOSE PARA CLUSTERING:")
print(f"   Â¿QuÃ© relaciÃ³n entre Income y Spending Score observas? La relaciÃ³n es que con mayor ingreso anual, mayor es el puntaje de compra.")
print(f"   Â¿Puedes imaginar grupos naturales de clientes? Si, clientes de bajo ingreso/bajo gasto, alto ingreso/alto gasto, y un grupo intermedio.")
```

```python
# === ANÃLISIS DE COLUMNAS DISPONIBLES ===
print("ANÃLISIS DE COLUMNAS PARA CLUSTERING:")
print(f"   Todas las columnas: {list(df_customers.columns)}")
print(f"   NumÃ©ricas: {df_customers.select_dtypes(include=[np.number]).columns.tolist()}")
print(f"   CategÃ³ricas: {df_customers.select_dtypes(include=[object]).columns.tolist()}")

# Identificar quÃ© excluir y quÃ© incluir
exclude_columns = ['CustomerID']  # ID no aporta informaciÃ³n
numeric_columns = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']
categorical_columns = ['Genre']

print(f"\nSELECCIÃ“N DE FEATURES:")
print(f"   Excluidas: {exclude_columns} (no informativas)")
print(f"   NumÃ©ricas: {numeric_columns}")
print(f"   CategÃ³ricas: {categorical_columns} (codificaremos)")
```

Aqui mostramos las columnas disponibles, y las clasificamos entre numÃ©ricas y categÃ³ricas, y a su vez excluimos las que no nos brindan informaciÃ³n de la persona en cuestiÃ³n.

```python
# === IMPORT ONEHOTENCODER ===
from sklearn.preprocessing import OneHotEncoder

print("CODIFICACIÃ“N DE VARIABLES CATEGÃ“RICAS CON SKLEARN:")
print("Usaremos OneHotEncoder en lugar de pd.get_dummies() por varias razones:")
print("   IntegraciÃ³n perfecta con pipelines de sklearn")
print("   Manejo automÃ¡tico de categorÃ­as no vistas en nuevos datos") 
print("   Control sobre nombres de columnas y comportamiento")
print("   Consistencia con el ecosistema de machine learning")

# Crear y configurar OneHotEncoder
encoder = OneHotEncoder(sparse_output=False)

# Ajustar y transformar Genre
genre_data = df_customers[['Genre']]  # Debe ser 2D para sklearn
genre_encoded_array = encoder.fit_transform(genre_data)  # MÃ©todo para ajustar y transformar

# Obtener nombres de las nuevas columnas
feature_names = encoder.get_feature_names_out(['Genre'])  # MÃ©todo para obtener nombres de las features
genre_encoded = pd.DataFrame(genre_encoded_array, columns=feature_names)

print(f"\nRESULTADO DE CODIFICACIÃ“N:")
print(f"   CategorÃ­as originales: {df_customers['Genre'].unique()}")
print(f"   Columnas generadas: {list(genre_encoded.columns)}")
print(f"   Shape: {genre_data.shape} â†’ {genre_encoded.shape}")

# Mostrar ejemplo de codificaciÃ³n
print(f"\nEJEMPLO DE TRANSFORMACIÃ“N:")
comparison = pd.concat([
    df_customers['Genre'].head().reset_index(drop=True),
    genre_encoded.head()
], axis=1)
print(comparison)
```

#### Resultados: OneHotEncoder
![Tabla comparativa](../assets/resultado-t6-parte1.8.png)

En esta seccion lo que buscamos es favorecer el modelo y para ello, creamos 2 categorias mÃ¡s a partir de los datos en sÃ­, Male/Female ahora tienen sus atributos Genre_Female y Genre_Male, las cuales son booleans indicando si son hombres o mujeres.

```python
# === CREACIÃ“N DEL DATASET FINAL ===
# Combinar variables numÃ©ricas + categÃ³ricas codificadas
X_raw = pd.concat([
    df_customers[numeric_columns],
    genre_encoded
], axis=1)

print("DATASET FINAL PARA CLUSTERING:")
print(f"   Shape: {X_raw.shape}")
print(f"   Columnas: {list(X_raw.columns)}")
print(f"   Variables numÃ©ricas: {numeric_columns}")
print(f"   Variables categÃ³ricas codificadas: {list(genre_encoded.columns)}")
print(f"   Total features: {X_raw.shape[1]} (3 numÃ©ricas + 2 categÃ³ricas binarias)")
print(f"   Memoria: {X_raw.memory_usage(deep=True).sum() / 1024:.1f} KB")

# === VERIFICACIONES ANTES DE CONTINUAR ===
print("VERIFICACIÃ“N DE CALIDAD:")

# 1. Datos faltantes
missing_data = X_raw.isnull().sum()
print(f"\nDATOS FALTANTES:")
if missing_data.sum() == 0:
    print("   PERFECTO! No hay datos faltantes")
else:
    for col, missing in missing_data.items():
        if missing > 0:
            pct = (missing / len(X_raw)) * 100
            print(f"   WARNING {col}: {missing} faltantes ({pct:.1f}%)")

# 2. Vista previa
print(f"\nVISTA PREVIA DEL DATASET:")
print(X_raw.head())

# 3. Tipos de datos
print(f"\nTIPOS DE DATOS:")
print(X_raw.dtypes)
```

#### Resultados: Nuevo DataSet
![Tabla comparativa](../assets/resultado-t6-parte1.9.png)

Creamos el dataset final con las variables creadas y numÃ©ricas que ya teniamos, y ademÃ¡s, hacemos un breve chequeo si tenemos que pulir el dataset en caso de nulos o informaciÃ³n faltante.

```python
# === ANÃLISIS DE ESCALAS ===
print("ANÃLISIS DE ESCALAS - Â¿Por quÃ© necesitamos normalizaciÃ³n?")

print(f"\nESTADÃSTICAS POR VARIABLE:")
for col in X_raw.columns:
    if X_raw[col].dtype in ['int64', 'float64']:  # Solo numÃ©ricas
        min_val = X_raw[col].min()
        max_val = X_raw[col].max()
        mean_val = X_raw[col].mean()
        std_val = X_raw[col].std()

        print(f"\n   {col}:")
        print(f"      Rango: {min_val:.1f} - {max_val:.1f}")
        print(f"      Media: {mean_val:.1f}")
        print(f"      DesviaciÃ³n: {std_val:.1f}")

print(f"\nANÃLISIS DE LAS ESTADÃSTICAS - COMPLETA:")
print(f"   Â¿QuÃ© variable tiene el rango mÃ¡s amplio? Annual Income (k$).")
print(f"   Â¿CuÃ¡l es la distribuciÃ³n de gÃ©nero en el dataset? Balanceada, un poquito mÃ¡s de mujeres.")
print(f"   Â¿QuÃ© variable muestra mayor variabilidad (std)? Annual Income (k$).")
print(f"   Â¿Los clientes son jÃ³venes o mayores en promedio? JÃ³venes-adultos, la edad promedio es de alrededor de 37 aÃ±os.")
print(f"   Â¿El income promedio sugiere quÃ© clase social? Clase media.")
print(f"   Â¿Por quÃ© la normalizaciÃ³n serÃ¡ crÃ­tica aca? Porque las variables estÃ¡n en escalas muy distintas, y puede llegar a sesgar los algoritmos de clustering.")

# Guardar para prÃ³ximas fases
feature_columns = list(X_raw.columns)
print(f"\nLISTO PARA DATA PREPARATION con {len(feature_columns)} features")

```

#### Resultados: anÃ¡lisis de estadÃ­sticas
![Tabla comparativa](../assets/resultado-t6-parte1.10.png)

## Parte 2: DescripciÃ³n
En esta parte buscamos iniciar la normalizaciÃ³n del dataset.

## Parte 2: CÃ³digo
```python
# === IMPORTAR HERRAMIENTAS DE NORMALIZACIÃ“N ===
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

print("BATALLA DE NORMALIZACIÃ“N: MinMax vs Standard vs Robust")
print("Objetivo: Encontrar el mejor scaler para nuestros datos")

# Recordar por quÃ© es importante
print(f"\nESCALAS ACTUALES (problema):")
for col in X_raw.columns:
    min_val, max_val = X_raw[col].min(), X_raw[col].max()
    print(f"   {col}: {min_val:.1f} - {max_val:.1f} (rango: {max_val-min_val:.1f})")

print("\nLas escalas son MUY diferentes - normalizaciÃ³n es crÃ­tica!")

```

#### Resultados: normalizaciÃ³n
![Tabla comparativa](../assets/resultado-t6-parte2.1.png)

En este anÃ¡lisis se revisaron los valores mÃ­nimos y mÃ¡ximos de cada variable y se vio que estÃ¡n en escalas muy distintas, por ejemplo, la edad llega hasta 70, los ingresos superan los 100 y las variables de gÃ©nero solo van de 0 a 1. Esta diferencia hace que unas variables tengan mucho mÃ¡s peso que otras en los cÃ¡lculos, por lo que es fundamental normalizar los datos para que todas esten en igualdad de condiciones a la hora de modelar.

```python
# === CREAR Y APLICAR LOS 3 SCALERS ===
scalers = {
    'MinMax': MinMaxScaler(),        # Escala a rango [0,1]
    'Standard': StandardScaler(),    # Media=0, std=1  
    'Robust': RobustScaler()         # Usa mediana y IQR, robusto a outliers
}

# Aplicar cada scaler
X_scaled = {}
for name, scaler in scalers.items():
    X_scaled[name] = scaler.fit_transform(X_raw)  # MÃ©todo para entrenar y transformar
    print(f"{name}Scaler aplicado: {X_scaled[name].shape}")

print(f"\nTenemos 3 versiones escaladas de los datos para comparar")

```

AquÃ­ la idea es aplicar 3 mÃ©todos de normalizaciÃ³n a los mismos registros para obtener 3 versiones paralelas del dataset, y de esta manera comprar cuÃ¡l funciona mejor en los siguientes anÃ¡lisis.

```python
# === COMPARACIÃ“N VISUAL CON BOXPLOTS ===
fig, axes = plt.subplots(1, 4, figsize=(16, 4))
fig.suptitle('ComparaciÃ³n de Scalers - Boxplots', fontsize=14, fontweight='bold')

# Datos originales
axes[0].boxplot([X_raw[col] for col in X_raw.columns], labels=X_raw.columns)
axes[0].set_title('Original')
axes[0].tick_params(axis='x', rotation=45)

# Datos escalados
for i, (name, X_scaled_data) in enumerate(X_scaled.items(), 1):
    axes[i].boxplot([X_scaled_data[:, j] for j in range(X_scaled_data.shape[1])], 
                    labels=X_raw.columns)
    axes[i].set_title(f'{name}')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

print("Observa cÃ³mo cada scaler ajusta las escalas de forma diferente")

```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte2.2.png)

AquÃ­ se usÃ³ boxplots para comparar visualmente el efecto de cada mÃ©todo de normalizaciÃ³n frente a los datos originales. El grÃ¡fico muestra cÃ³mo las variables tenÃ­an rangos muy distintos al inicio y al aplicar cada scaler, quedan ajustadas a escalas mÃ¡s comparables. Con MinMax todas las variables se llevan entre 0 y 1, con Standard se centran en media 0 con dispersiÃ³n ajustada a la desviaciÃ³n estÃ¡ndar y con Robust se ve un ajuste similar pero afecta diferente a los xtremos ya que son menores que el anterior.

```python
# === COMPARAR DISTRIBUCIONES DE UNA VARIABLE ===
# Vamos a analizar 'Annual Income (k$)' en detalle
income_col_idx = 1  # PosiciÃ³n de Annual Income

fig, axes = plt.subplots(1, 4, figsize=(16, 4))
fig.suptitle('Annual Income: Original vs Scalers', fontsize=14, fontweight='bold')

# Original
axes[0].hist(X_raw.iloc[:, income_col_idx], bins=20, alpha=0.7, color='gray', edgecolor='black')
axes[0].set_title('Original')
axes[0].set_xlabel('Annual Income (k$)')

# Escalados
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
for i, ((name, X_scaled_data), color) in enumerate(zip(X_scaled.items(), colors), 1):
    axes[i].hist(X_scaled_data[:, income_col_idx], bins=20, alpha=0.7, color=color, edgecolor='black')
    axes[i].set_title(f'{name}')
    axes[i].set_xlabel('Annual Income (escalado)')

plt.tight_layout()
plt.show()

print("Â¿Notas cÃ³mo cambia la forma de la distribuciÃ³n?")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte2.3.png)

AquÃ­ se comparÃ³ la variable Annual Income en su forma original y despuÃ©s de aplicar los tres mÃ©todos de normalizaciÃ³n. Se puede apreciar en los histogramas que la forma de la distribuciÃ³n se mantiene en todos los casos, pero cambia la escala en que se representa, con MinMax entre 0 y 1, con Standard entre -2 y 3, y con Robust entre -1 y 2 aprox.

```python
# === ESTADÃSTICAS DESPUÃ‰S DEL SCALING ===
print("ESTADÃSTICAS POST-SCALING (Annual Income):")

# Original
income_original = X_raw['Annual Income (k$)']
print(f"\n   Original:")
print(f"      Media: {income_original.mean():.1f}")
print(f"      Std:   {income_original.std():.1f}")
print(f"      Min:   {income_original.min():.1f}")
print(f"      Max:   {income_original.max():.1f}")

# Escalados
for name, X_scaled_data in X_scaled.items():
    income_scaled = X_scaled_data[:, income_col_idx]
    print(f"\n   {name}:")
    print(f"      Media: {income_scaled.mean():.3f}")
    print(f"      Std:   {income_scaled.std():.3f}")
    print(f"      Min:   {income_scaled.min():.3f}")
    print(f"      Max:   {income_scaled.max():.3f}")

print(f"\nOBSERVACIONES:")
print(f"   MinMaxScaler â†’ Rango [0,1]")
print(f"   StandardScaler â†’ Media=0, Std=1")
print(f"   RobustScaler â†’ Menos afectado por outliers")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte2.4.png)

Se calcula para Annual Income las estadÃ­sticas antes y despuÃ©s de las normalizaciones; la media, la desviaciÃ³n estÃ¡ndar, el valor mÃ­nimo y el mÃ¡ximo en la escala original, y despuÃ©s nuevamente pero tras aplicar cada scaler.

```python
# === IMPORT PARA CLUSTERING TEST ===
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# === QUICK TEST: Â¿QuÃ© scaler funciona mejor para clustering? ===
print("QUICK TEST: Impacto en Clustering (K=4)")

clustering_results = {}
for name, X_scaled_data in X_scaled.items():
    # Aplicar K-Means con K=4
    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)  # Completar
    labels = kmeans.fit_predict(X_scaled_data)  # MÃ©todo para obtener clusters

    # Calcular silhouette score
    silhouette = silhouette_score(X_scaled_data, labels)  # MÃ©trica de calidad
    clustering_results[name] = silhouette

    print(f"   {name:>10}: Silhouette Score = {silhouette:.3f}")

# Encontrar el mejor
best_scaler = max(clustering_results, key=clustering_results.get)
best_score = clustering_results[best_scaler]

print(f"\nGANADOR: {best_scaler} (Score: {best_score:.3f})")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte2.5.png)

El test de clustering muestra que al usar K-Means con 4 clusters, el MinMaxScaler produjo el mejor score con 0.364, seguido por Standard 0.332 y Robust 0.298. Esto indica que para este dataset en particular, llevar todas las variables al rango 0..1 permite que el algoritmo identifique mÃ¡s claramente los clusters.

```python
# === TOMAR DECISIÃ“N BASADA EN RESULTADOS ===
print("DECISIÃ“N FINAL DEL SCALER:")

print(f"\nCOMPLETE TU ANÃLISIS:")
print(f"   Mejor scaler segÃºn silhouette: {best_scaler}")
print(f"   Â¿Por quÃ© crees que funcionÃ³ mejor? Porque MinMax ajusta todas las variables al mismo rango 0..1, lo que ayuda a que K-Means mida mejor las distancias y detecte clusters mÃ¡s claros.")
print(f"   Â¿AlgÃºn scaler tuvo problemas obvios? RobustScaler tuvo menor score, probablemente porque suaviza los outliers y reduce la diferencia de rangos.")

# Implementar decisiÃ³n
selected_scaler_name = best_scaler  # O elige manualmente: 'MinMax', 'Standard', 'Robust'
selected_scaler = scalers[selected_scaler_name]

# Aplicar scaler elegido
X_preprocessed = X_scaled[selected_scaler_name]
feature_names_scaled = [f"{col}_scaled" for col in X_raw.columns]

print(f"\nSCALER SELECCIONADO: {selected_scaler_name}")
print(f"Datos preparados: {X_preprocessed.shape}")
print(f"Listo para PCA y Feature Selection")

```

Afirmamos que MinMax fue el mejor porque pone todas las variables en la misma escala, y que RobustScaler es el que tuvo mÃ¡s problemas.

```python
from sklearn.decomposition import PCA

# === OPERACIÃ“N: DIMENSION COLLAPSE ===
print("PCA: Reduciendo dimensiones sin perder la esencia")
print("   Objetivo: De 5D â†’ 2D para visualizaciÃ³n + anÃ¡lisis de varianza")

# 1. Aplicar PCA completo para anÃ¡lisis de varianza
pca_full = PCA()
X_pca_full = pca_full.fit_transform(X_preprocessed)

# 2. ANÃLISIS DE VARIANZA EXPLICADA
explained_variance_ratio = pca_full.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

print(f"\nğŸ“Š ANÃLISIS DE VARIANZA EXPLICADA:")
for i, (var, cum_var) in enumerate(zip(explained_variance_ratio, cumulative_variance)):
    print(f"   PC{i+1}: {var:.3f} ({var*100:.1f}%) | Acumulada: {cum_var:.3f} ({cum_var*100:.1f}%)")

# 3. VISUALIZACIÃ“N DE VARIANZA EXPLICADA
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Scree plot
axes[0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 
           alpha=0.7, color='#FF6B6B')
axes[0].set_xlabel('Componentes Principales')
axes[0].set_ylabel('Varianza Explicada')
axes[0].set_title('ğŸ“Š Scree Plot - Varianza por Componente')
axes[0].set_xticks(range(1, len(explained_variance_ratio) + 1))

# Cumulative variance
axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 
            marker='o', linewidth=2, markersize=8, color='#4ECDC4')
axes[1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% threshold')
axes[1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% threshold')
axes[1].set_xlabel('NÃºmero de Componentes')
axes[1].set_ylabel('Varianza Acumulada')
axes[1].set_title('ğŸ“ˆ Varianza Acumulada')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_xticks(range(1, len(cumulative_variance) + 1))

plt.tight_layout()
plt.show()

# 4. DECISIÃ“N SOBRE NÃšMERO DE COMPONENTES
print(f"\nğŸ¯ DECISIÃ“N DE COMPONENTES:")
n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1
n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1

print(f"   ğŸ“Š Para retener 90% varianza: {n_components_90} componentes")
print(f"   ğŸ“Š Para retener 95% varianza: {n_components_95} componentes")
print(f"   ğŸ¯ Para visualizaciÃ³n: 2 componentes ({cumulative_variance[1]*100:.1f}% varianza)")

# 5. APLICAR PCA CON 2 COMPONENTES PARA VISUALIZACIÃ“N
pca_2d = PCA(n_components=2, random_state=42)
X_pca_2d = pca_2d.fit_transform(X_preprocessed)

print(f"\nPCA aplicado:")
print(f"   ğŸ“Š Dimensiones: {X_preprocessed.shape} â†’ {X_pca_2d.shape}")
print(f"   ğŸ“ˆ Varianza explicada: {pca_2d.explained_variance_ratio_.sum()*100:.1f}%")

# 6. ANÃLISIS DE COMPONENTES PRINCIPALES
print(f"\nğŸ” INTERPRETACIÃ“N DE COMPONENTES:")
feature_names = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)', 'Genre_Female', 'Genre_Male']

for i, pc in enumerate(['PC1', 'PC2']):
    print(f"\n   {pc} (varianza: {pca_2d.explained_variance_ratio_[i]*100:.1f}%):")
    # Obtener los loadings (pesos de cada feature original en el componente)
    loadings = pca_2d.components_[i]
    for j, (feature, loading) in enumerate(zip(feature_names, loadings)):
        direction = "â†‘" if loading > 0 else "â†“"
        print(f"     {feature:>15}: {loading:>7.3f} {direction}")

# 7. VISUALIZACIÃ“N EN 2D
plt.figure(figsize=(12, 8))
plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.6, s=50, color='#96CEB4')
plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% varianza)')
plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% varianza)')
plt.title('Mall Customers en Espacio PCA 2D')
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nğŸ’¡ INTERPRETACIÃ“N DE NEGOCIO:")
print(f"   ğŸ¯ PC1 parece representar: diferenciaciÃ³n de gÃ©nero, hombres vs mujeres dentro del dataset.")
print(f"   ğŸ¯ PC2 parece representar: age vs spending score, es decir, cÃ³mo el comportamiento de compra cambia segpun la edad.")
print(f"   ğŸ“Š Los clusters visibles sugieren: grupos marcados como jÃ³venes con gasto alto, adultos con gasto bajo y diferenciados por gÃ©nero, mujeres con gastos mÃ¡s altos que los hombres.")
```

#### Resultados: PCA
![Tabla comparativa](../assets/resultado-t6-parte2.6.png)

![Tabla comparativa](../assets/resultado-t6-parte2.7.png)

El PCA nos ayudÃ³ a simplificar el dataset de 5 variables a 2 componentes claves que mantienen mÃ¡s del 85% de la informaciÃ³n. Esto nos muestra patrones importantes en el dataset, por ejemplo, el gÃ©nero es un factor muy dominante, seguido por la relaciÃ³n edadâ€“gasto, lo que ayudarÃ­a a pensar en estrategias de marketing mÃ¡s personalizadas.

## Parte 3: DescripciÃ³n
Se definirÃ¡ la configuraciÃ³n bÃ¡sica para poder probar quÃ© subconjunto de caracterÃ­sticas aporta mÃ¡s valor al modelo. La idea es reducir el nÃºmero de variables sin perder informaciÃ³n importante, lo que hace que el modelo sea mÃ¡s simple, interpretable y eficiente.

## Parte 3: Setup inicial

```python
# === IMPORTS PARA FEATURE SELECTION ===
from sklearn.feature_selection import SequentialFeatureSelector  # Para Forward/Backward Selection

# === OPERACIÃ“N: FEATURE SELECTION SHOWDOWN ===
print("ğŸ¯ FEATURE SELECTION vs PCA: Â¿Seleccionar o Transformar?")
print("   ğŸ¯ Objetivo: Comparar Forward/Backward Selection vs PCA")

print(f"\nğŸ“Š FEATURE SELECTION: Forward vs Backward vs PCA")
print(f"   Dataset: {X_preprocessed.shape[0]} muestras, {X_preprocessed.shape[1]} features")

# Setup: FunciÃ³n para evaluar features en clustering
def evaluate_features_for_clustering(X, n_clusters=4):
    """EvalÃºa quÃ© tan buenas son las features para clustering usando Silhouette Score"""
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X)
    return silhouette_score(X, labels)

# === IMPORTS PARA ESTIMADORES PERSONALIZADOS ===
from sklearn.base import BaseEstimator, ClassifierMixin  # Clases base necesarias

# CLASE AUXILIAR: Estimador basado en KMeans para SequentialFeatureSelector
class ClusteringEstimator(BaseEstimator, ClassifierMixin):
    """Estimador que usa KMeans y Silhouette Score para feature selection"""
    def __init__(self, n_clusters=4):
        self.n_clusters = n_clusters

    def fit(self, X, y=None):
        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        self.labels_ = self.kmeans_.fit_predict(X)
        return self

    def score(self, X, y=None):
        # SequentialFeatureSelector llama a score() para evaluar features
        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        return silhouette_score(X, labels)

    def predict(self, X):
        # MÃ©todo requerido por ClassifierMixin
        if hasattr(self, 'kmeans_'):
            return self.kmeans_.predict(X)
        else:
            # Si no estÃ¡ entrenado, entrenar primero
            kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
            return kmeans.fit_predict(X)

print("âœ… Setup completado - Funciones de evaluaciÃ³n listas")
```

En este anÃ¡lisis comparamos Feature Selection y PCA: en la primera buscamos elegir las variables originales mÃ¡s relevantes (Forward o Backward), mientras que en la segunda creamos nuevas variables transformadas que concentran la varianza. La idea es ver quÃ© tÃ©cnica resulta mÃ¡s Ãºtil para simplificar un dataset de 200 muestras con 5 caracterÃ­sticas.

## Parte 3: CÃ³digo

```python
# BASELINE: Todas las features
baseline_score = evaluate_features_for_clustering(X_preprocessed)
print(f"\nğŸ“Š BASELINE (todas las features): Silhouette = {baseline_score:.3f}")
print(f"   Este es el score con las {X_preprocessed.shape[1]} features originales")
print(f"   Â¿Podremos mejorar seleccionando solo las mejores 3?")

# === FORWARD SELECTION (sklearn oficial) ===
print(f"\nğŸ”„ FORWARD SELECTION (sklearn oficial):")
print(f"   Estrategia: Empezar con 0 features, agregar la mejor en cada paso")

forward_selector = SequentialFeatureSelector(
    estimator=ClusteringEstimator(n_clusters=4),  # Estimador que implementa fit() y score()
    n_features_to_select=3,
    direction='forward',  # Para Forward Selection
    cv=3,
    n_jobs=-1
)

forward_selector.fit(X_preprocessed)  # MÃ©todo para entrenar
forward_mask = forward_selector.get_support()  # MÃ©todo para obtener mÃ¡scara booleana
X_forward = X_preprocessed[:, forward_mask]
forward_features = np.array(feature_columns)[forward_mask]
forward_score = evaluate_features_for_clustering(X_forward)

print(f"   Features seleccionadas: {list(forward_features)}")
print(f"   ğŸ“Š Silhouette Score: {forward_score:.3f}")
print(f"   {'âœ… Mejora!' if forward_score > baseline_score else 'âŒ Sin mejora'}")

# === BACKWARD ELIMINATION (sklearn oficial) ===
print(f"\nğŸ”„ BACKWARD ELIMINATION (sklearn oficial):")
print(f"   Estrategia: Empezar con todas las features, eliminar la peor en cada paso")

backward_selector = SequentialFeatureSelector(
    estimator=ClusteringEstimator(n_clusters=4),  # Mismo estimador que Forward
    n_features_to_select=3,
    direction='backward',  # Para Backward Selection
    cv=3,
    n_jobs=-1
)

backward_selector.fit(X_preprocessed)  # MÃ©todo para entrenar
backward_mask = backward_selector.get_support()  # MÃ©todo para obtener mÃ¡scara
X_backward = X_preprocessed[:, backward_mask]
backward_features = np.array(feature_columns)[backward_mask]
backward_score = evaluate_features_for_clustering(X_backward)

print(f"   Features seleccionadas: {list(backward_features)}")
print(f"   ğŸ“Š Silhouette Score: {backward_score:.3f}")
print(f"   {'âœ… Mejora!' if backward_score > baseline_score else 'âŒ Sin mejora'}")

```

En este bloque de cÃ³digo buscamos simplificar el dataset para mejorar el clustering. PCA transforma las variables originales en nuevas combinaciones, mientras que Forward Selection empieza sin features y va agregando progresivamente las mÃ¡s Ãºtiles, y Backward Elimination empieza con todas y va quitando las menos relevantes. En cada caso lo que buscamos es quedarnos sÃ³lo con la informaciÃ³n mÃ¡s relevante, de manera que permita identificar mejor los grupos de clientes.

```python
# === COMPARACIÃ“N FINAL DE TODOS LOS MÃ‰TODOS ===
print(f"\nğŸ“Š COMPARACIÃ“N DE MÃ‰TODOS:")
print(f"   ğŸ Baseline (todas): {baseline_score:.3f}")
print(f"   ğŸ”„ Forward Selection: {forward_score:.3f}")
print(f"   ğŸ”™ Backward Elimination: {backward_score:.3f}")

# Comparar con PCA (ya calculado anteriormente)
pca_score = evaluate_features_for_clustering(X_pca_2d)
print(f"   ğŸ“ PCA (2D): {pca_score:.3f}")

# Encontrar el mejor mÃ©todo
methods = {
    'Baseline (todas)': baseline_score,
    'Forward Selection': forward_score, 
    'Backward Elimination': backward_score,
    'PCA (2D)': pca_score
}

best_method = max(methods, key=methods.get)
best_score = methods[best_method]

print(f"\nğŸ† GANADOR: {best_method} con score = {best_score:.3f}")

# AnÃ¡lisis de diferencias
print(f"\nğŸ” ANÃLISIS:")
for method, score in sorted(methods.items(), key=lambda x: x[1], reverse=True):
    improvement = ((score - baseline_score) / baseline_score) * 100
    print(f"   {method}: {score:.3f} ({improvement:+.1f}% vs baseline)")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte3.1.png)

En esta comparaciÃ³n final vemos cÃ³mo distintos mÃ©todos de simplificaciÃ³n de variables impactan el clustering. Usanndo todas las features da un baseline de 0.364, y tanto Forward Selection como Backward Elimination mejoran el score a 0.573, +57,3%. Sin embargo, PCA 2D logra el mejor resultado ya que con 0.686, +68%, transforma las variables en componentes que mejora la estructura de los datos. En definitiva, transformarlas con PCA demostrÃ³ ser la estrategia mÃ¡s efectiva para separar grupos de clientes.

```python
# === VISUALIZACIÃ“N DE COMPARACIÃ“N ===
methods_names = ['Baseline', 'Forward', 'Backward', 'PCA 2D'] 
scores_values = [baseline_score, forward_score, backward_score, pca_score]
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

plt.figure(figsize=(12, 6))
bars = plt.bar(methods_names, scores_values, color=colors, alpha=0.7)
plt.ylabel('Silhouette Score')
plt.title('ComparaciÃ³n de MÃ©todos de Feature Selection')
plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Threshold Aceptable (0.5)')
plt.axhline(y=0.7, color='green', linestyle='--', alpha=0.5, label='Threshold Muy Bueno (0.7)')
plt.legend()
plt.grid(True, alpha=0.3, axis='y')

# AÃ±adir valores en las barras
for bar, score in zip(bars, scores_values):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte3.2.png)

```python
# === ANÃLISIS DE RESULTADOS ===
print(f"\nğŸ¯ ANÃLISIS DE RESULTADOS:")

# Comparar features seleccionadas
print(f"\nğŸ” FEATURES SELECCIONADAS POR CADA MÃ‰TODO:")
print(f"   ğŸ”„ Forward Selection: {list(forward_features)}")
print(f"   ğŸ”™ Backward Elimination: {list(backward_features)}")

# AnÃ¡lisis de coincidencias
forward_set = set(forward_features)
backward_set = set(backward_features)

common_forward_backward = forward_set & backward_set

print(f"\nğŸ¤ COINCIDENCIAS:")
print(f"   Forward âˆ© Backward: {list(common_forward_backward)}")
print(f"   Â¿Seleccionaron las mismas features? {'SÃ­' if forward_set == backward_set else 'No'}")

# FILL-IN-THE-BLANKS: Preguntas de anÃ¡lisis
print(f"\nâ“ PREGUNTAS DE ANÃLISIS (completa):")
print(f"   ğŸ’¡ MÃ©todo con mejor score: PCA 2D")  # Respuesta en base a los resultados
print(f"   ğŸ“Š Â¿Forward y Backward seleccionaron exactamente las mismas features? No")
print(f"   ğŸ¤” Â¿PCA con 2 componentes es competitivo? SÃ­, fue el que obtuvo mayor Silhouette Score") 
print(f"   ğŸ¯ Â¿AlgÃºn mÃ©todo superÃ³ el threshold de 0.5? SÃ­, Forward, Backward y PCA")
print(f"   ğŸ“ˆ Â¿La reducciÃ³n de dimensionalidad mejorÃ³ el clustering? SÃ­, especialmente PCA")

```
#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte3.3.png)

```python
# === DECISIÃ“N PARA EL ANÃLISIS FINAL ===
print(f"\nğŸ¢ DECISIÃ“N PARA EL ANÃLISIS:")

# Decidir mÃ©todo basado en resultados
if best_score == pca_score:
    selected_method = "PCA"
    selected_data = X_pca_2d
    print(f"   ğŸ¯ SELECCIONADO: PCA (2D) - Score: {pca_score:.3f}")
    print(f"   âœ… RAZÃ“N: Mejor balance entre reducciÃ³n dimensional y performance")
elif best_score == forward_score:
    selected_method = "Forward Selection" 
    selected_data = X_forward
    print(f"   ğŸ¯ SELECCIONADO: Forward Selection - Score: {forward_score:.3f}")
    print(f"   âœ… RAZÃ“N: Mejor score con features interpretables")
elif best_score == backward_score:
    selected_method = "Backward Elimination"
    selected_data = X_backward  
    print(f"   ğŸ¯ SELECCIONADO: Backward Elimination - Score: {backward_score:.3f}")
    print(f"   âœ… RAZÃ“N: Mejor score eliminando features redundantes")
else:
    # Fallback to baseline if needed
    selected_method = "Baseline (todas las features)"
    selected_data = X_preprocessed
    print(f"   ğŸ¯ SELECCIONADO: Baseline - Score: {baseline_score:.3f}")
    print(f"   âœ… RAZÃ“N: NingÃºn mÃ©todo de reducciÃ³n mejorÃ³ el clustering")

# Guardar para clustering final
X_final_for_clustering = selected_data
final_method_name = selected_method

print(f"\nğŸ“Š PREPARADO PARA CLUSTERING:")
print(f"   MÃ©todo: {final_method_name}")
print(f"   Dimensiones: {X_final_for_clustering.shape}")
print(f"   Silhouette Score: {best_score:.3f}")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte3.4.png)

Se decidiÃ³ usar PCA con 2 componentes para el anÃ¡lisis de clustering, ya que ofreciÃ³ el mejor balance entre reducir dimensionalidad y mantener un buen desempeÃ±o, con un Silhouette Score de 0.686.


## Parte 4: DescripciÃ³n
En esta fase vamos a crear los grupos de clientes a partir de los datos que preprocesamos. Vamos a usar K-Means para encontrar los clusters naturales y ver cÃ³mo se agrupan los clientes, con el objetivo de entender mejor sus comportamientos y poder pensar en estrategias de marketing mÃ¡s dirigidas.

## Parte 4: CÃ³digo
```python
# === OPERACIÃ“N: CUSTOMER SEGMENTATION DISCOVERY ===
print("K-MEANS CLUSTERING: Descubriendo segmentos de clientes")
print(f"   Dataset: {X_final_for_clustering.shape} usando mÃ©todo '{final_method_name}'")

# 1. BÃšSQUEDA DEL K Ã“PTIMO - Elbow Method + Silhouette
print(f"\nğŸ“ˆ BÃšSQUEDA DEL K Ã“PTIMO:")

k_range = range(2, 9)
inertias = []
silhouette_scores = []

for k in k_range:
    # Aplicar K-Means
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_final_for_clustering)

    # Calcular mÃ©tricas
    inertias.append(kmeans.inertia_)
    sil_score = silhouette_score(X_final_for_clustering, labels)
    silhouette_scores.append(sil_score)

    print(f"   K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={sil_score:.3f}")

# 2. VISUALIZACIÃ“N ELBOW METHOD + SILHOUETTE
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Elbow Method
axes[0].plot(k_range, inertias, marker='o', linewidth=2, markersize=8, color='#FF6B6B')
axes[0].set_xlabel('NÃºmero de Clusters (K)')
axes[0].set_ylabel('Inertia (WCSS)')
axes[0].set_title('ğŸ“ˆ Elbow Method')
axes[0].grid(True, alpha=0.3)
axes[0].set_xticks(k_range)

# Silhouette Scores
axes[1].plot(k_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='#4ECDC4')
axes[1].axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Aceptable (0.5)')
axes[1].axhline(y=0.7, color='green', linestyle='--', alpha=0.7, label='Muy Bueno (0.7)')
axes[1].set_xlabel('NÃºmero de Clusters (K)')
axes[1].set_ylabel('Silhouette Score')
axes[1].set_title('ğŸ“Š Silhouette Analysis')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].set_xticks(k_range)

plt.tight_layout()
plt.show()

# 3. ANÃLISIS DEL ELBOW METHOD
print(f"\nğŸ§  ELBOW METHOD - DEEP DIVE ANALYSIS:")
print(f"\nğŸ“‰ **Â¿QuÃ© es exactamente 'el codo'?**")
print(f"   - **MatemÃ¡ticamente:** Punto donde la segunda derivada de WCSS vs K cambia mÃ¡s dramÃ¡ticamente")
print(f"   - **Visualmente:** Donde la curva pasa de 'caÃ­da empinada' a 'caÃ­da suave'")
print(f"   - **Conceptualmente:** Balance entre simplicidad (menos clusters) y precisiÃ³n (menor error)")

# Calcular diferencias para encontrar el codo
differences = np.diff(inertias)
second_differences = np.diff(differences)
elbow_candidate = k_range[np.argmin(second_differences) + 2]  # +2 por los dos diff()

print(f"\nğŸ“Š **AnÃ¡lisis cuantitativo del codo:**")
for i, k in enumerate(k_range[:-2]):
    print(f"   K={k}: Î” Inertia={differences[i]:.2f}, Î”Â²={second_differences[i]:.2f}")

print(f"\nğŸ¯ **Candidato por Elbow Method:** K={elbow_candidate}")

# 4. DECISIÃ“N FINAL DE K
best_k_silhouette = k_range[np.argmax(silhouette_scores)]
print(f"ğŸ¯ **Candidato por Silhouette:** K={best_k_silhouette} (score={max(silhouette_scores):.3f})")

print(f"\nğŸ¤ **DECISIÃ“N FINAL:**")
if elbow_candidate == best_k_silhouette:
    optimal_k = elbow_candidate
    print(f"   Ambos mÃ©todos coinciden: K = {optimal_k}")
else:
    print(f"   âš–ï¸  Elbow sugiere K={elbow_candidate}, Silhouette sugiere K={best_k_silhouette}")
    print(f"   ğŸ’¼ Considerando el contexto de negocio (3-5 segmentos esperados)...")
    # Elegir basado en contexto de negocio y calidad
    if 3 <= best_k_silhouette <= 5 and max(silhouette_scores) > 0.4:
        optimal_k = best_k_silhouette
        print(f"   Elegimos K = {optimal_k} (mejor silhouette + contexto negocio)")
    else:
        optimal_k = elbow_candidate if 3 <= elbow_candidate <= 5 else 4
        print(f"   Elegimos K = {optimal_k} (balance elbow + contexto negocio)")

# 5. MODELO FINAL CON K Ã“PTIMO
print(f"\nğŸ¯ ENTRENANDO MODELO FINAL CON K={optimal_k}")

final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)
final_labels = final_kmeans.fit_predict(X_final_for_clustering)
final_silhouette = silhouette_score(X_final_for_clustering, final_labels)

print(f"Modelo entrenado:")
print(f"   ğŸ“Š Silhouette Score: {final_silhouette:.3f}")
print(f"   ğŸ¯ Clusters encontrados: {optimal_k}")
print(f"   ğŸ“ˆ Inertia final: {final_kmeans.inertia_:.2f}")

# 6. DISTRIBUCIÃ“N DE CLIENTES POR CLUSTER
cluster_counts = pd.Series(final_labels).value_counts().sort_index()
print(f"\nğŸ‘¥ DISTRIBUCIÃ“N DE CLIENTES:")
for cluster_id, count in cluster_counts.items():
    percentage = (count / len(final_labels)) * 100
    print(f"   Cluster {cluster_id}: {count:,} clientes ({percentage:.1f}%)")

# 7. AGREGAR CLUSTERS AL DATAFRAME ORIGINAL
df_customers['cluster'] = final_labels
df_customers['cluster_name'] = df_customers['cluster'].map({
    i: f"Cluster_{i}" for i in range(optimal_k)
})

print(f"\nClusters asignados al dataset original")
```
#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte4.1.png)

![Tabla comparativa](../assets/resultado-t6-parte4.2.png)

En este paso aplicamos K-Means para probar distintos valores de K y despuÃ©s evaluar cuÃ¡l era el mÃ¡s adecuado usando dos tÃ©cnicas, Elbow Method y Silhouette Score. Con esto vimos que, aunque K=2 tenÃ­a la mayor separaciÃ³n, K=4 daba un mejor equilibrio entre la calidad de los grupos y la utilidad prÃ¡ctica para segmentar a los clientes. Esto permitiÃ³ identificar cuÃ¡ntos clusters usar y confirmar que 4 grupos era la mejor opciÃ³n para seguir adelante.

## Parte 5: DescripciÃ³n
En esta fase vamos a crear los grupos de clientes a partir de los datos que preprocesamos. Vamos a usar K-Means para encontrar los clusters naturales y ver cÃ³mo se agrupan los clientes, con el objetivo de entender mejor sus comportamientos y poder pensar en estrategias de marketing mÃ¡s dirigidas.

## Parte 5: CÃ³digo
```python
# === OPERACIÃ“N: INTELLIGENCE REPORT ===
print("ANALISIS DE SEGMENTOS DE CLIENTES - REPORTE EJECUTIVO")

# 1. PERFILES DE CLUSTERS
print(f"\nPERFILES DETALLADOS POR CLUSTER:")

for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_data = df_customers[df_customers['cluster'] == cluster_id]
    cluster_size = len(cluster_data)

    print(f"\n**CLUSTER {cluster_id}** ({cluster_size} clientes, {cluster_size/len(df_customers)*100:.1f}%)")

    # EstadÃ­sticas usando las columnas CORRECTAS del Mall Customer Dataset
    avg_age = cluster_data['Age'].mean()
    avg_income = cluster_data['Annual Income (k$)'].mean()
    avg_spending = cluster_data['Spending Score (1-100)'].mean()

    # DistribuciÃ³n por gÃ©nero
    genre_counts = cluster_data['Genre'].value_counts()

    print(f"   **Perfil DemogrÃ¡fico:**")
    print(f"      Edad promedio: {avg_age:.1f} aÃ±os")
    print(f"      DistribuciÃ³n gÃ©nero: {dict(genre_counts)}")

    print(f"   **Perfil Financiero:**")
    print(f"      Ingreso anual: ${avg_income:.1f}k")
    print(f"      Spending Score: {avg_spending:.1f}/100")

    # Comparar con ground truth si estÃ¡ disponible
    if 'true_segment' in df_customers.columns:
        true_segments_in_cluster = cluster_data['true_segment'].value_counts()
        dominant_segment = true_segments_in_cluster.index[0]
        purity = true_segments_in_cluster.iloc[0] / cluster_size
        print(f"   ğŸ¯ **Ground Truth:** {dominant_segment} ({purity*100:.1f}% purity)")

# 2. MATRIZ DE CONFUSIÃ“N CON GROUND TRUTH
if 'true_segment' in df_customers.columns:
    print(f"\nğŸ¯ VALIDACIÃ“N CON GROUND TRUTH:")
    confusion_matrix = pd.crosstab(df_customers['true_segment'], df_customers['cluster'], 
                                  margins=True, margins_name="Total")
    print(confusion_matrix)

    # Calcular pureza de clusters
    cluster_purities = []
    for cluster_id in sorted(df_customers['cluster'].unique()):
        cluster_data = df_customers[df_customers['cluster'] == cluster_id]
        dominant_true_segment = cluster_data['true_segment'].mode().iloc[0]
        purity = (cluster_data['true_segment'] == dominant_true_segment).mean()
        cluster_purities.append(purity)

    average_purity = np.mean(cluster_purities)
    print(f"\nğŸ“Š Pureza promedio de clusters: {average_purity:.3f}")

# 3. VISUALIZACIÃ“N DE CLUSTERS
if final_method_name == 'PCA':  # Si usamos PCA, podemos visualizar en 2D
    plt.figure(figsize=(15, 10))

    # Subplot 1: Clusters encontrados
    plt.subplot(2, 2, 1)
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
    for cluster_id in sorted(df_customers['cluster'].unique()):
        cluster_mask = final_labels == cluster_id
        plt.scatter(X_pca_2d[cluster_mask, 0], X_pca_2d[cluster_mask, 1], 
                   c=colors[cluster_id % len(colors)], label=f'Cluster {cluster_id}',
                   alpha=0.7, s=50)

    # Plotear centroides
    if final_method_name == 'PCA':
        centroids_pca = final_kmeans.cluster_centers_
        plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], 
                   c='red', marker='X', s=200, linewidths=3, label='Centroides')

    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.title('Clusters Descubiertos (PCA 2D)')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Subplot 2: Ground truth (si disponible)
    if 'true_segment' in df_customers.columns:
        plt.subplot(2, 2, 2)
        true_segment_colors = {'VIP': '#FF6B6B', 'Regular': '#4ECDC4', 
                              'Occasional': '#45B7D1', 'At_Risk': '#96CEB4'}
        for segment, color in true_segment_colors.items():
            segment_mask = df_customers['true_segment'] == segment
            segment_indices = df_customers[segment_mask].index
            plt.scatter(X_pca_2d[segment_indices, 0], X_pca_2d[segment_indices, 1], 
                       c=color, label=segment, alpha=0.7, s=50)

        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.title('Ground Truth Segments')
        plt.legend()
        plt.grid(True, alpha=0.3)

    # Subplot 3: Feature distribution by cluster
    plt.subplot(2, 2, 3)
    # Usar las columnas correctas del Mall Customer Dataset
    cluster_means = df_customers.groupby('cluster')[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].mean()
    cluster_means.plot(kind='bar', ax=plt.gca(), color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
    plt.title('Perfil Promedio por Cluster')
    plt.ylabel('Valor Promedio')
    plt.legend(title='CaracterÃ­sticas', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.xticks(rotation=0)

    # Subplot 4: Cluster sizes
    plt.subplot(2, 2, 4)
    cluster_sizes = df_customers['cluster'].value_counts().sort_index()
    colors_subset = [colors[i] for i in cluster_sizes.index]
    bars = plt.bar(cluster_sizes.index, cluster_sizes.values, color=colors_subset, alpha=0.7)
    plt.xlabel('Cluster ID')
    plt.ylabel('NÃºmero de Clientes')
    plt.title('DistribuciÃ³n de Clientes por Cluster')

    # AÃ±adir etiquetas en las barras
    for bar, size in zip(bars, cluster_sizes.values):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, 
                f'{size}\n({size/len(df_customers)*100:.1f}%)', 
                ha='center', va='bottom')

plt.tight_layout()
plt.show()
```
#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte5.1.png)

![Tabla comparativa](../assets/resultado-t6-parte5.2.png)

Los 4 clusters muestran patrones marcados, tenemos dos grupos jÃ³venes (Clusters 0 y 3, aprox 28 aÃ±os) con alto Spending Score de entre 68..70 pero divididos por gÃ©nero (0 para las mujeres y 3 para hombres), y dos grupos de mayores (Clusters 1 y 2, aprox entre 48 y 50 aÃ±os) con gasto bajo entre 30..35, tambiÃ©n separados por gÃ©nero (1 para hombres, 2 para mujeres). Los ingresos medios son parecidos entre clusters 59..62k, por lo tanto la diferencia principal viene de la edad, el gasto y el gÃ©nero.

```python
# === ANÃLISIS SILHOUETTE POR CLUSTER ===
print(f"\nğŸ“Š ANÃLISIS SILHOUETTE DETALLADO:")

from sklearn.metrics import silhouette_samples  # FunciÃ³n para silhouette por muestra individual

# Calcular silhouette score por muestra
sample_silhouette_values = silhouette_samples(X_final_for_clustering, final_labels)

# EstadÃ­sticas por cluster
print(f"   ğŸ¯ Silhouette Score General: {final_silhouette:.3f}")
for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_silhouette_values = sample_silhouette_values[final_labels == cluster_id]
    cluster_avg_silhouette = cluster_silhouette_values.mean()
    cluster_min_silhouette = cluster_silhouette_values.min()

    print(f"   Cluster {cluster_id}: Î¼={cluster_avg_silhouette:.3f}, "
          f"min={cluster_min_silhouette:.3f}, "
          f"samples={len(cluster_silhouette_values)}")

```
#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte5.3.png)

El anÃ¡lisis de silhouette dio un score general de 0.686, lo que significa que los grupos quedaron bastante bien formados. El Cluster 3 es el mÃ¡s fuerte (Î¼=0.759), aunque tiene algunos clientes medio descolgados. Los Clusters 0 y 2 andan parejos con valores cercanos a 0.67, mientras que el Cluster 1 es el mÃ¡s flojo (Î¼=0.659). En resumen, la segmentaciÃ³n saliÃ³ bien, pero hay unos pocos casos que no encajan del todo en su grupo.

```python
# === DETECCIÃ“N DE OUTLIERS EN CLUSTERING ===
print(f"\nğŸš¨ DETECCIÃ“N DE OUTLIERS EN CLUSTERING:")
outlier_threshold = 0.0  # Silhouette negativo = mal asignado

for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_mask = final_labels == cluster_id
    cluster_silhouette = sample_silhouette_values[cluster_mask]
    outliers = np.sum(cluster_silhouette < outlier_threshold)

    if outliers > 0:
        print(f"   âš ï¸  Cluster {cluster_id}: {outliers} posibles outliers (silhouette < 0)")
else:
        print(f"   âœ… Cluster {cluster_id}: Sin outliers detectados")

```
#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte5.4.png)

En la detecciÃ³n de outliers se revisÃ³ si habÃ­a clientes con silhouette negativo, es decir, mal asignados a su grupo. El resultado mostrÃ³ que en el Cluster 3 no se encontraron outliers, lo que indica que todos los clientes en ese grupo estÃ¡n bien representados y encajan con el perfil del cluster.

```python
# === ANÃLISIS DE PERFILES POR CLUSTER ===
print(f"\nANALISIS DE SEGMENTOS DE CLIENTES - REPORTE EJECUTIVO")
print(f"\nPERFILES DETALLADOS POR CLUSTER:")

# AnÃ¡lisis por cluster usando las columnas REALES del dataset
for cluster_id in sorted(df_customers['cluster'].unique()):
    cluster_data = df_customers[df_customers['cluster'] == cluster_id]
    cluster_size = len(cluster_data)
    cluster_pct = (cluster_size / len(df_customers)) * 100

    # EstadÃ­sticas usando las columnas CORRECTAS del Mall Customer Dataset
    avg_age = cluster_data['Age'].mean()
    avg_income = cluster_data['Annual Income (k$)'].mean()
    avg_spending = cluster_data['Spending Score (1-100)'].mean()

    # DistribuciÃ³n por gÃ©nero
    genre_counts = cluster_data['Genre'].value_counts()

    print(f"\nğŸ·ï¸  **CLUSTER {cluster_id}** ({cluster_size} clientes, {cluster_pct:.1f}%)")
    print(f"   ğŸ“Š **Perfil DemogrÃ¡fico:**")
    print(f"      ğŸ‘¤ Edad promedio: {avg_age:.1f} aÃ±os")
    print(f"      ğŸ‘¥ DistribuciÃ³n gÃ©nero: {dict(genre_counts)}")

    print(f"   ğŸ’° **Perfil Financiero:**")
    print(f"      ğŸ’µ Ingreso anual: ${avg_income:.1f}k")
    print(f"      ğŸ›ï¸  Spending Score: {avg_spending:.1f}/100")
```
#### Resultados
![Tabla comparativa](../assets/resultado-t6-parte5.5.png)

Este anÃ¡lisis de perfiles por cluster muestra cuatro segmentos bien diferenciados de clientes. El Cluster 0 esta conformado por mujeres jÃ³venes con ingresos medios y alto nivel de gasto, el Cluster 1 son hombres mayores con ingresos altos pero bajo gasto, el Cluster 2 incluye mujeres adultas con ingresos medios y un gasto mÃ¡s controlado, y por ultimo el Cluster 3 se compone de hombres jÃ³venes con ingresos altos y propenso al consumo. Con esto se entiende mejor cÃ³mo se comportan distintos grupos y sirve como base para diseÃ±ar estrategias de marketing personalizadas.

## ğŸ“ Reflexiones de Data Detective

ğŸ” MetodologÃ­a CRISP-DM:
#### Â¿QuÃ© fase fue mÃ¡s desafiante y por quÃ©?
##### Data preparation, porque implicÃ³ probar distintos escaladores y reducir dimensionalidad sin perder informaciÃ³n.
#### Â¿CÃ³mo el entendimiento del negocio influyÃ³ en tus decisiones tÃ©cnicas?
##### Me ayudÃ³ a priorizar tÃ©cnicas que generaran segmentos Ãºtiles para marketing, no solo buenos scores.

ğŸ§¹ Data Preparation:
#### Â¿QuÃ© scaler funcionÃ³ mejor y por quÃ©?
##### PCA con datos escalados dio el mejor rendimiento, mostrando que la normalizaciÃ³n ayudaba a comparar variables en la misma escala.
#### Â¿PCA o Feature Selection fue mÃ¡s efectivo para tu caso?
##### PCA fue mÃ¡s efectivo porque logrÃ³ un mejor Silhouette Score.
#### Â¿CÃ³mo balanceaste interpretabilidad vs performance?
##### ElegÃ­ PCA por performance, aunque con menor interpretabilidad en las variables originales.

ğŸ§© Clustering:
#### Â¿El Elbow Method y Silhouette coincidieron en el K Ã³ptimo?
##### SÃ­, ambos seÃ±alaron que 4 clusters era una buena elecciÃ³n.
#### Â¿Los clusters encontrados coinciden con la intuiciÃ³n de negocio?
##### SÃ­, reflejan perfiles de clientes diferenciados (jÃ³venes gastadores, mayores conservadores, etc.).
#### Â¿QuÃ© harÃ­as diferente si fueras a repetir el anÃ¡lisis?
##### ProbarÃ­a otros algoritmos de clustering como DBSCAN o jerÃ¡rquico para comparar resultados.

ğŸ’¼ AplicaciÃ³n PrÃ¡ctica:
#### Â¿CÃ³mo presentarÃ­as estos resultados en un contexto empresarial?
##### Con grÃ¡ficos simples y perfiles claros de clientes para que sea entendible por marketing.
#### Â¿QuÃ© valor aportan estas segmentaciones?
##### Permiten diseÃ±ar estrategias personalizadas y optimizar campaÃ±as.
#### Â¿QuÃ© limitaciones tiene este anÃ¡lisis?
##### El dataset es pequeÃ±o y simplificado, en la prÃ¡ctica se necesitarÃ­an mÃ¡s variables y datos reales.


## ğŸ§¬ Challenge 1: Algoritmos de Clustering Alternativos
En este challenge usamos diferentes algoritmos de clustering mÃ¡s allÃ¡ de K-Means, la idea es comprender cÃ³mo se comportan frente a estructuras de datos complejas. El objetivo es usar DBSCAN, HDBSCAN, Gaussian Mixture Models, Agglomerative Clustering y Spectral Clustering, comparando su capacidad para identificar grupos, manejar ruido y adaptarse a distintos patrones.

## Challenge 1: CÃ³digo
```python
# === DBSCAN: Encuentra clusters de densidad arbitraria ===
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors

print("DBSCAN: Clustering basado en densidad")

# 1. Encontrar parÃ¡metros Ã³ptimos
def find_optimal_eps(X, min_samples=5):
    """Encuentra eps Ã³ptimo usando k-distance graph"""
    nbrs = NearestNeighbors(n_neighbors=min_samples).fit(X)
    distances, indices = nbrs.kneighbors(X)
    distances = np.sort(distances[:, min_samples-1], axis=0)

    # Plotear k-distance graph
    plt.figure(figsize=(8, 5))
    plt.plot(distances)
    plt.xlabel('Data Points sorted by distance')
    plt.ylabel(f'{min_samples}-NN distance')
    plt.title('K-distance Graph for DBSCAN eps selection')
    plt.grid(True, alpha=0.3)
    plt.show()

    return distances

# Encontrar eps
distances = find_optimal_eps(X_final_for_clustering)
optimal_eps = 0.5 # Â¿QuÃ© valor elegirÃ­as del grÃ¡fico?

# Aplicar DBSCAN
dbscan = DBSCAN(eps=optimal_eps, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_final_for_clustering)

# AnÃ¡lisis de resultados
n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise_points = list(dbscan_labels).count(-1)

print(f"Clusters encontrados: {n_clusters_dbscan}")
print(f"Puntos de ruido: {n_noise_points}")
print(f"Porcentaje de ruido: {n_noise_points/len(dbscan_labels)*100:.1f}%")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto1.1.png)

Se detectÃ³ 2 clusters bien definidos y no clasificÃ³ ningÃºn punto como ruido, lo que indica que la densidad de los datos estaba distribuida de manera clara. A diferencia de K-Means, aquÃ­ los grupos no fueron impuestos por un nÃºmero fijo de K, sino que emergieron de la estructura de los datos.

```python
# === HDBSCAN: VersiÃ³n jerÃ¡rquica de DBSCAN ===
# !pip install hdbscan  # Instalar si no estÃ¡ disponible

import hdbscan

print("HDBSCAN: Clustering jerÃ¡rquico basado en densidad")

# Aplicar HDBSCAN
hdbscan_clusterer = hdbscan.HDBSCAN(min_cluster_size=5,  # TamaÃ±o mÃ­nimo de cluster
                                   min_samples=5,        # Muestras mÃ­nimas por cluster
                                   metric='euclidean')

hdbscan_labels = hdbscan_clusterer.fit_predict(X_final_for_clustering)

# VisualizaciÃ³n del Ã¡rbol de clustering
hdbscan_clusterer.condensed_tree_.plot(select_clusters=True)
plt.title('HDBSCAN Condensed Tree')
plt.show()

print(f"Clusters HDBSCAN: {hdbscan_clusterer.labels_.max() + 1}")
print(f"Cluster persistence: {hdbscan_clusterer.cluster_persistence_}")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto1.2.png)

Este modelo detectÃ³ 10 clusters en los datos, cada uno con diferentes niveles de persistencia. Los valores muestran quÃ© tan estables son los clusters, por ejemplo, unos tienen una consistencia buena, 0.44 o 0.36, pero otros tienen muy baja estabilidad, 0.002 o 0.027, lo que indicarÃ­a que podrÃ­an ser ruido.

```python
# === GMM: Clustering probabilÃ­stico ===
from sklearn.mixture import GaussianMixture

print("Gaussian Mixture Models: Clustering probabilÃ­stico")

# Encontrar nÃºmero Ã³ptimo de componentes
n_components_range = range(2, 8)
aic_scores = []
bic_scores = []

for n_components in n_components_range:
    gmm = GaussianMixture(n_components=n_components, random_state=42)
    gmm.fit(X_final_for_clustering)
    aic_scores.append(gmm.aic(X_final_for_clustering))
    bic_scores.append(gmm.bic(X_final_for_clustering))

# Plot AIC/BIC
plt.figure(figsize=(10, 5))
plt.plot(n_components_range, aic_scores, 'o-', label='AIC')
plt.plot(n_components_range, bic_scores, 's-', label='BIC')
plt.xlabel('Number of components')
plt.ylabel('Information Criterion')
plt.title('GMM Model Selection: AIC vs BIC')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Aplicar GMM Ã³ptimo
optimal_n_components = n_components_range[np.argmin(bic_scores)]
gmm = GaussianMixture(n_components=optimal_n_components, random_state=42)
gmm_labels = gmm.fit_predict(X_final_for_clustering)
gmm_probabilities = gmm.predict_proba(X_final_for_clustering)

print(f"Componentes Ã³ptimos (BIC): {optimal_n_components}")
print(f"Log-likelihood: {gmm.score(X_final_for_clustering):.3f}")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto1.3.png)

El GMM diÃ³ que lo mejor era usar 4 componentes segÃºn el BIC, es decir, que los datos se entienden mejor si los pensamos como cuatro grupos gaussianos distintos. AdemÃ¡s, el log-likelihood saliÃ³ 3.307, lo que bÃ¡sicamente quiere decir que el modelo se adaptÃ³ bastante bien a cÃ³mo estÃ¡n distribuidos los datos.


```python
# === SPECTRAL CLUSTERING: Clustering en espacio espectral ===
from sklearn.cluster import SpectralClustering, AgglomerativeClustering

print("Spectral Clustering: Clustering en espacio transformado")

spectral = SpectralClustering(n_clusters=optimal_k, 
                             affinity='rbf',  # Â¿rbf, nearest_neighbors, o precomputed?
                             random_state=42)

spectral_labels = spectral.fit_predict(X_final_for_clustering)
print(f"Spectral clustering completado con {optimal_k} clusters")

# === AGGLOMERATIVE CLUSTERING ===
agglomerative = AgglomerativeClustering(n_clusters=optimal_k,
                                       linkage='ward')  # ward, complete, average, single

agglo_labels = agglomerative.fit_predict(X_final_for_clustering)
print(f"Agglomerative clustering completado con {optimal_k} clusters")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto1.4.png)

Probamos Spectral y Agglomerative Clustering para ver los grupos desde enfoques distintos. Los dos dieron 4 clusters, igual que otros mÃ©todos, lo que confirma que los grupos que encontramos son bastante consistentes y confiables.

## ğŸ”„ Challenge 2: Recursive Feature Elimination (RFE)
En este challenge usamos RFE (Recursive Feature Elimination) para encontrar cuÃ¡les features realmente aportan a separar bien los clusters. La idea es ir probando de a poco, sacando features menos relevantes y viendo cÃ³mo afecta a la calidad de los grupos con K-Means y el Silhouette Score. BÃ¡sicamente, es como depurar las variables hasta quedarnos solo con las que hacen que los clusters se vean mÃ¡s claros y definidos.

## Challenge 2: CÃ³digo
```python
# === RFE: Feature Selection Recursivo ===
from sklearn.feature_selection import RFE
from sklearn.base import BaseEstimator, ClassifierMixin

print("RECURSIVE FEATURE ELIMINATION: SelecciÃ³n iterativa de features")

# Clase auxiliar para RFE con clustering
class RFEClusteringEstimator(BaseEstimator, ClassifierMixin):
    """Estimador para RFE que usa KMeans + Silhouette"""
    def __init__(self, n_clusters=4):
        self.n_clusters = n_clusters

    def fit(self, X, y=None):
        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        self.labels_ = self.kmeans_.fit_predict(X)
        # RFE requiere feature_importances_ o coef_
        self.feature_importances_ = self._calculate_feature_importance(X)
        return self

    def _calculate_feature_importance(self, X):
        """Calcula importancia usando varianza intra-cluster vs inter-cluster"""
        importances = []
        for i in range(X.shape[1]):
            feature_values = X[:, i]

            # Varianza total
            total_var = np.var(feature_values)

            # Varianza intra-cluster (promedio ponderado)
            intra_cluster_var = 0
            for cluster_id in range(self.n_clusters):
                cluster_mask = self.labels_ == cluster_id
                if np.sum(cluster_mask) > 1:  # Al menos 2 puntos en el cluster
                    cluster_var = np.var(feature_values[cluster_mask])
                    cluster_weight = np.sum(cluster_mask) / len(feature_values)
                    intra_cluster_var += cluster_var * cluster_weight

            # Importancia: ratio de separaciÃ³n entre clusters
            if total_var > 0:
                importance = 1 - (intra_cluster_var / total_var)
            else:
                importance = 0

            importances.append(max(0, importance))  # No negativo

        return np.array(importances)

    def score(self, X, y=None):
        kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        return silhouette_score(X, labels)

    def predict(self, X):
        if hasattr(self, 'kmeans_'):
            return self.kmeans_.predict(X)
        else:
            kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)
            return kmeans.fit_predict(X)

# Aplicar RFE
print("Aplicando RFE para encontrar las mejores features...")

rfe_estimator = RFEClusteringEstimator(n_clusters=4)
rfe = RFE(estimator=rfe_estimator, 
          n_features_to_select=3,  # Seleccionar top 3 features
          step=1)  # Eliminar 1 feature por iteraciÃ³n

y_dummy = np.zeros(X_preprocessed.shape[0])
rfe.fit(X_preprocessed, y=y_dummy)
X_rfe = rfe.transform(X_preprocessed)
rfe_features = np.array(feature_names)[rfe.support_]
rfe_score = evaluate_features_for_clustering(X_rfe)

print(f"Features seleccionadas por RFE: {list(rfe_features)}")
print(f"Silhouette Score RFE: {rfe_score:.3f}")
print(f"Ranking de features: {dict(zip(feature_names, rfe.ranking_))}")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto2.png)

RFE encontrÃ³ que las tres features mÃ¡s importantes para separar bien los clusters eran Age, Genre_Female y Genre_Male. El Silhouette Score quedÃ³ en 0.637, asÃ­ que los clusters se ven bastante definidos y no estÃ¡n demasiado mezclados. El ranking nos muestra que Annual Income y Spending Score ayudan, pero no tanto como la edad y el gÃ©nero, asÃ­ que principalmente estas tres son las principales para simplificar el anÃ¡lisis sin perder mucha info.

## ğŸ“Š Challenge 3: Datasets Alternativos
En este challenge probamos nuestro pipeline de clustering con tres datasets distintos para ver cÃ³mo se comporta con datos reales y sintÃ©ticos.

## Challenge 3: CÃ³digo
```python
# === IRIS DATASET ===
from sklearn.datasets import load_iris

print("IRIS DATASET: El clÃ¡sico dataset de flores")

iris = load_iris()
X_iris = iris.data
y_iris_true = iris.target  # Ground truth para validaciÃ³n

print(f"Iris shape: {X_iris.shape}")
print(f"Features: {iris.feature_names}")
print(f"Especies: {iris.target_names}")

# Aplicar pipeline completo en Iris
scaler_iris = StandardScaler()
X_iris_scaled = scaler_iris.fit_transform(X_iris)

pca_iris = PCA(n_components=2)
X_iris_pca = pca_iris.fit_transform(X_iris_scaled)

# Clustering en Iris
kmeans_iris = KMeans(n_clusters=3, random_state=42)
iris_clusters = kmeans_iris.fit_predict(X_iris_pca)

# ComparaciÃ³n con ground truth
from sklearn.metrics import adjusted_rand_score  # Adjusted Rand Index
ari_score = adjusted_rand_score(y_iris_true, iris_clusters)
print(f"Adjusted Rand Index vs ground truth: {ari_score:.3f}")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto3.1.png)

El clustering en Iris quedÃ³ decente: con un ARI de 0.433 separÃ³ mÃ¡s o menos bien las especies, pero todavÃ­a hay mezcla entre versicolor y virginica. BÃ¡sicamente K-Means captÃ³ cierta estructura, aunque no es perfecto, habrÃ­a que probar otros mÃ©todos o features para levantar el score.


```python
# === WINE DATASET ===
from sklearn.datasets import load_wine

wine = load_wine()
X_wine = wine.data
y_wine_true = wine.target

print(f"Wine Dataset shape: {X_wine.shape}")
print(f"Features: {wine.feature_names[:5]}...")  # Primeras 5 features
print(f"Clases de vino: {wine.target_names}")

# Tu anÃ¡lisis completo aca...

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

# Escalar datos
scaler = StandardScaler()
X_wine_scaled = scaler.fit_transform(X_wine)

# Reducir a 2 componentes principales para ver mejor
pca = PCA(n_components=2)
X_wine_pca = pca.fit_transform(X_wine_scaled)
print(f"Varianza explicada con 2 PCs: {pca.explained_variance_ratio_.sum():.3f}")

# KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
wine_clusters = kmeans.fit_predict(X_wine_pca)

# Comparar con etiquetas reales
ari_score = adjusted_rand_score(y_wine_true, wine_clusters)
print(f"ARI KMeans vs ground truth: {ari_score:.3f}")
```

![Tabla comparativa](../assets/resultado-t6-reto3.2.png)

El dataset de vinos tiene 178 muestras con 13 caracterÃ­sticas quÃ­micas. Al reducir la dimensionalidad con PCA a 2 componentes, logramos explicar un 55% de la varianza, lo cual ya nos da una buena idea de cÃ³mo se distribuyen los datos. DespuÃ©s de aplicar KMeans con 3 clusters, el ARI dio 0.895, lo que significa que el clustering se ajustÃ³ bastante bien con las clases reales de vino. En definitiva, las features quÃ­micas sÃ­ diferencian bastante bien los tipos de vino.

```python
# === SYNTHETIC BLOBS ===
from sklearn.datasets import make_blobs

# Generar datos sintÃ©ticos con caracterÃ­sticas conocidas
X_blobs, y_blobs_true = make_blobs(n_samples=300, 
                                  centers=4, 
                                  n_features=2,  # Â¿CuÃ¡ntas dimensiones?
                                  cluster_std=1.0,  # Â¿QuÃ© dispersiÃ³n?
                                  random_state=42)

print(f"Synthetic blobs shape: {X_blobs.shape}")

# Â¿Puede tu pipeline detectar los 4 clusters correctamente?

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

# Escalar
scaler = StandardScaler()
X_blobs_scaled = scaler.fit_transform(X_blobs)

# PCA a 2D (ya son 2D, pero mantenemos el formato)
pca = PCA(n_components=2)
X_blobs_pca = pca.fit_transform(X_blobs_scaled)

# Clustering
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
blobs_clusters = kmeans.fit_predict(X_blobs_pca)

# ComparaciÃ³n con ground truth
ari_score = adjusted_rand_score(y_blobs_true, blobs_clusters)
print(f"ARI KMeans vs ground truth (blobs): {ari_score:.3f}")
```

![Tabla comparativa](../assets/resultado-t6-reto3.3.png)

Para los Synthetic Blobs generamos un dataset controlado con 4 clusters bien separados y 2 dimensiones. El objetivo probar si nuestro pipeline de clustering podÃ­a reconocerlos sin complicaciones, y despuÃ©s de escalar y aplicar PCA, KMeans detectÃ³ los clusters y comparando con la â€œverdadâ€ del dataset, el ARI dio 0.991, casi perfecto. BÃ¡sicamente casi todos los puntos quedaron en el cluster correcto.


## ğŸ¨ Challenge 4: VisualizaciÃ³n Avanzada


## Challenge 4: CÃ³digo
```python

```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto4.1.png)

![Tabla comparativa](../assets/resultado-t6-reto4.2.png)

![Tabla comparativa](../assets/resultado-t6-reto4.3.png)



## ğŸ“ˆ Challenge 5: ComparaciÃ³n Masiva de Algoritmos


## Challenge 5: CÃ³digo
```python

```

#### Resultados
![Tabla comparativa](../assets/resultado-t6-reto5.png)