---
title: "Pr√°ctica 14"
date: 2025-11-11
---

# Pr√°ctica 14
## üî• LLMs con LangChain (OpenAI)

## Contexto
En esta pr√°ctica se busc√≥ familiarizarse con el uso de modelos de chat de OpenAI dentro del ecosistema LangChain, entendiendo c√≥mo configurar el modelo, controlar su comportamiento mediante par√°metros de decodificaci√≥n y dise√±ar prompts robustos orientados a flujos reutilizables. Adem√°s, se introdujo el concepto de salidas estructuradas y observabilidad mediante medici√≥n de tokens y latencia, pilares fundamentales para construir aplicaciones conversacionales confiables y escalables.

## Objetivos
- Instanciar un modelo de chat de OpenAI desde LangChain (ChatOpenAI) y realizar invocaciones b√°sicas.
- Controlar par√°metros de decodificaci√≥n comunes (temperature, max_tokens, top_p) y razonar sobre su efecto.
- Dise√±ar prompts reutilizables con ChatPromptTemplate y el LCEL para encadenar componentes.
- Obtener salidas estructuradas (JSON/Pydantic) de forma fiable con with_structured_output.
- Medir tokens/latencia con tracing en LangSmith (o callbacks) como base de observabilidad.

## Actividades (con tiempos estimados)
- **Parte 0: SetUp** (10 min)
Configuraci√≥n del entorno, instalaci√≥n de dependencias y prueba inicial de importaciones de LangChain y OpenAI.

- **Parte 1: Par√°metros** (20 min)
Instanciaci√≥n del modelo ChatOpenAI y experimentaci√≥n con temperature, top_p y max_tokens para observar su efecto en la generaci√≥n.

- **Parte 2: De texto suelto a plantillas** (20 min)
Conversi√≥n de prompts escritos manualmente en estructuras formales mediante ChatPromptTemplate, separando sistema, usuario y variables.

- **Parte 3: Salida estructurada** (30 min)
Uso de with_structured_output y modelos Pydantic para obtener respuestas controladas y libres de errores de formato.

- **Parte 4: M√©tricas m√≠nimas: tokens y latencia** (20 min)
Registro de tokens usados, costo aproximado y tiempo de respuesta utilizando LangSmith o callbacks locales.

- **Parte 5: Mini-tareas guiadas** (25 min)
Implementaci√≥n de peque√±as funciones (resumir, corregir tono, clasificar) encadenadas con LCEL para practicar modularidad.

- **Parte 6: Zero-shot vs Few-shot** (20 min)
Comparaci√≥n directa de resultados con y sin ejemplos, analizando mejoras en precisi√≥n y reducci√≥n de alucinaciones.

- **Parte 7: Res√∫menes: single-doc y multi-doc** (25 min)
Creaci√≥n de pipelines de resumen simple y estilo map-reduce, probando cambios en chunking, prompts y temperatura.

- **Parte 8: Extracci√≥n de informaci√≥n** (20 min)
Definici√≥n de entidades y esquemas para extraer fechas, organizaciones, personas y eventos mediante output estructurado.

- **Parte 9: RAG b√°sico con textos locales** (30 min)
Construcci√≥n de un pipeline RAG minimalista con prompt de combinaci√≥n y prueba con distintos k.

## Desarrollo
Se configur√≥ un modelo de chat de OpenAI utilizando ChatOpenAI, verificando primero su funcionamiento b√°sico. Luego se experiment√≥ con distintos par√°metros de decodificaci√≥n para observar su impacto en creatividad, coherencia y longitud de la respuesta. Posteriormente, se dise√±aron prompts modulares con ChatPromptTemplate, permitiendo separar instrucciones del contenido variable. Como siguiente paso, se aplic√≥ with_structured_output para obtener respuestas estrictamente formateadas mediante Pydantic, reduciendo errores y mejorando la confiabilidad. Finalmente, se conect√≥ todo en un flujo LCEL y se habilit√≥ trazado para analizar tokens, tiempo de inferencia y variantes de configuraci√≥n.

## Evidencias
- Se adjuntan imagenes desde **"resultado-t14-1.png"** a **"resultado-t14-11.png"** en `docs/assets/`

## Reflexi√≥n
La pr√°ctica permiti√≥ ver c√≥mo LangChain facilita no solo la invocaci√≥n de modelos, sino tambi√©n la construcci√≥n de pipelines limpios, trazables y robustos. Tambi√©n los par√°metros de decodificaci√≥n demostraron ser clave para obtener control fino sobre el estilo de generaci√≥n, mientras que las salidas estructuradas evidenciaron la importancia de la validaci√≥n y consistencia en aplicaciones reales.

---

### Parte 0: SetUp

```python
# Instalaci√≥n (Colab/Local)
!pip install -U "langchain>=0.2.11" "langchain-core>=0.2.33" "langchain-community>=0.2.11" "langchain-openai>=0.2.1" "langsmith>=0.1.97"
# Opcionales para el assignment:
!pip install -U faiss-cpu chromadb tavily-python duckduckgo-search langchain-text-splitters

import os
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = ""  # configurar tu key
os.environ["OPENAI_API_KEY"] = "sk-proj--"  # configurar tu key
```

Configuramos las keys e instalamos todo lo necesario. Cabe marcar que no pondr√© las keys autenticas en github por seguridad.

#### Hello LLM
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-5-mini", temperature=0)  # modelo sugerido

# Hola LLM
resp = llm.invoke("Defin√≠ 'Transformer' en una sola oraci√≥n.")
print(resp.content)
```
#### Resultado: LLM
![Tabla comparativa](../assets/resultado-t14-1.png)

La respuesta del modelo es clara y correcta porque en una sola oraci√≥n explica que un Transformer usa atenci√≥n para procesar secuencias de manera eficiente, cumpliendo exactamente con lo que ped√≠a el prompt LLM.

```python
from langchain_openai import ChatOpenAI

# Completar par√°metros b√°sicos (ver [1] y [12])
MODEL = "gpt-5-mini"         # p.ej., "gpt-5-mini"
TEMP = 0.5           # 0.0‚Äì1.0 (determinismo vs creatividad)

llm = ChatOpenAI(model=MODEL, temperature=TEMP)
print(llm.invoke("Hola! Decime tu versi√≥n en una l√≠nea.").content)
```

#### Resultados
##### Temp 0.5
![Tabla comparativa](../assets/resultado-t14-2.png)
##### Temp 0
![Tabla comparativa](../assets/resultado-t14-3.png)
##### Temp 0.9
![Tabla comparativa](../assets/resultado-t14-4.png)

El modelo siempre devolvi√≥ un mensaje present√°ndose como "ChatGPT, modelo GPT-4o", pero lo que cambia en cada temp distinta es que con mayor temp su creatividad en la respuesta es mayor y cambia un poco, aunque la idea central de la respuesta sea la misma.

## Reflexi√≥n

#### ¬øQu√© cambia si ped√≠s 1 vs 3 oraciones?
##### Cuando le pido 1 oraci√≥n, la respuesta es m√°s directa porque el modelo intenta meter toda la idea en una sola frase. En cambio, cuando pido 3 oraciones, el modelo suele dividir la explicaci√≥n en partes m√°s claras y detalladas, como si estuviera haciendo una mini explicaci√≥n paso a paso.

#### ¬øObserv√°s variancia entre ejecuciones con la misma consigna?
##### S√≠, especialmente cuando la temperatura no es 0. Si ejecuto el mismo prompt varias veces, el modelo cambia el estilo, el vocabulario, aunque el contenido general sea parecido. Con temperature 0 casi no cambia, pero con valores m√°s altos la variaci√≥n se nota bastante.

### Parte 1: Par√°metros

```python
prompts = [
    "Escrib√≠ un tuit (<=20 palabras) celebrando un paper de IA.",
    "Dame 3 bullets concisos sobre ventajas de los Transformers."
]

for t in [0.0, 0.5, 0.9]:
    llm_t = ChatOpenAI(model="gpt-5-mini", temperature=t)
    outs = [llm_t.invoke(p).content for p in prompts]
    print(f"\n--- Temperature={t} ---")
    for i, o in enumerate(outs, 1):
        print(f"[{i}] {o}")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t14-5.png)

Cuando prob√© las tres temperaturas, se not√≥ que las respuestas cambian bastante, con 0.0 el modelo siempre responde casi igual, s√∫per literal y sin creatividad, con 0.5 ya aparece un poco m√°s de variedad y suena m√°s natural, y con 0.9 las respuestas son m√°s libres, m√°s creativas y hasta un poco impredecibles. Tambi√©n vi que si le pido 1 oraci√≥n, la respuesta suele ser m√°s estable entre ejecuciones, pero cuando pido 3 oraciones aumenta la variaci√≥n porque el modelo tiene m√°s espacio para cambiar el estilo o agregar detalles. En general, s√≠ hay variancia incluso con la misma consigna, y cuanto mayor es la temperatura o la cantidad de texto pedido, m√°s se nota.

```python
from langchain_openai import ChatOpenAI

MODEL = "gpt-5-mini"
TEMP = 0      # 0.0, 0.5, 0.9

llm = ChatOpenAI(model=MODEL, temperature=TEMP)
print(llm.invoke("Escrib√≠ un haiku sobre evaluaci√≥n de modelos.").content)
```

#### Resultados
##### Temp 0.0
![Tabla comparativa](../assets/resultado-t14-6.png)
##### Temp 0.5
![Tabla comparativa](../assets/resultado-t14-7.png)
##### Temp 0.9
![Tabla comparativa](../assets/resultado-t14-8.png)

Aqu√≠ vemos un poco m√°s de lo mismo, el resultado muestra c√≥mo cambia la creatividad del modelo seg√∫n la temperatura. Con temp 0, el haiku es m√°s r√≠gido y directo, casi como si siguiera una f√≥rmula fija. Con temp 0.5, aparece m√°s variedad en las palabras y la idea se siente un poco m√°s po√©tica. Y con temp 0.9, el haiku suena m√°s libre y expresivo, como si el modelo experimentara m√°s con el lenguaje.

## Reflexi√≥n

#### ¬øQu√© combinaci√≥n te da claridad vs creatividad?
##### Las temperaturas bajas, como 0 a 0.3 dan respuestas m√°s claras, directas y consistentes. Las temperaturas medias o altas, 0.6 a 1.0 producen m√°s creatividad, variaci√≥n y estilos menos predecibles.

#### ¬øC√≥mo impactan estos par√°metros en tareas ‚Äúcerradas‚Äù (respuestas √∫nicas)?
##### En tareas cerradas, una temperatura baja funciona mejor porque disminuye la variabilidad y evita desv√≠os o inventos. Con temperaturas altas, el modelo puede agregar detalles innecesarios o responder de forma menos precisa, lo que afecta la exactitud en problemas donde solo hay una respuesta correcta.


### Parte 2: De texto suelto a plantillas

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "Sos un asistente conciso, exacto y profesional."),
    ("human",  "Explic√° {tema} en <= 3 oraciones, con un ejemplo real.")
])

chain = prompt | llm  # LCEL: prompt ‚Üí LLM
print(chain.invoke({"tema": "atenci√≥n multi-cabeza"}).content)
```

#### Resultados
> "La atenci√≥n multi‚Äëcabeza proyecta consultas, claves y valores en varias subrepresentaciones (cabezas), calcula atenci√≥n por producto punto escalado en cada una y concatena sus resultados para capturar distintos tipos de relaciones en paralelo. T√©cnicamente cada cabeza aprende a destacar patrones diferentes (p. ej. sintaxis, sem√°ntica, posiciones) y la proyecci√≥n final combina esa informaci√≥n. Ejemplo real: en traducci√≥n, una cabeza puede alinear l√©xicamente "bank"‚Üí"banco", otra resolver concordancia gramatical y otra identificar que "it" se refiere a "the cat", mejorando precisi√≥n y coherencia."

El modelo sigui√≥ perfecto el template, dio una explicaci√≥n corta con tres oraciones y us√≥ un ejemplo concreto. El tono qued√≥ t√©cnico y profesional porque el mensaje system se lo marc√≥. La explicaci√≥n repasa qu√© hace la atenci√≥n multi-cabeza, por qu√© sirve y el ejemplo de traducci√≥n muestra c√≥mo cada cabeza puede enfocarse en algo diferente.

## Reflexi√≥n

#### ¬øCu√°ndo conviene few-shot vs instrucciones claras?
##### Conviene usar few-shot cuando quer√©s que el modelo copie un estilo, una forma de razonar o un formato espec√≠fico que ser√≠a dif√≠cil explicar solo con instrucciones. En cambio, usar instrucciones claras sirve mejor cuando la tarea es simple, bien definida o tiene una respuesta esperada muy directa.

#### ¬øC√≥mo cambia el formato cuando el template fija estructura?
##### Cuando el template fija una estructura, el modelo responde mucho m√°s ordenado y predecible, se ve que sigue el formato exacto, evita divagar y es m√°s f√°cil validar la salida porque ya est√° ‚Äúencajada‚Äù en el molde que le marcaste.


### Parte 3: Salida estructurada 

```python
from typing import List
from pydantic import BaseModel

class Resumen(BaseModel):
    title: str
    bullets: List[str]

llm_json = llm.with_structured_output(Resumen)  # garantiza JSON v√°lido que cumple el esquema

pedido = "Resum√≠ en 3 bullets los riesgos de la 'prompt injection' en LLM apps."
res = llm_json.invoke(pedido)
res
```

#### Resultados
![Tabla comparativa](../assets/resultado-t14-9.png)

Se devolvi√≥ un objeto con el formato exacto que le pedimos, un t√≠tulo y tres bullets. Cada bullet explica un riesgo t√≠pico de prompt injection, pero usando un lenguaje m√°s t√©cnico: filtrar datos, hacer que el modelo se comporte mal y saltarse controles de seguridad.

## Reflexi√≥n

#### ¬øQu√© mejora frente a ‚Äúparsear a mano‚Äù cadenas JSON?
##### Usar with_structured_output mejora mucho frente a parsear JSON a mano porque el modelo ya devuelve directamente un objeto v√°lido que sigue el esquema. Esto evita errores comunes como JSON mal formado, claves faltantes y problemas de casteo, y te ahorra escribir l√≥gica extra para validar y corregir la respuesta.

#### ¬øQu√© contratos de salida necesit√°s en producci√≥n?
##### En producci√≥n necesit√°s contratos de salida bien definidos, campos fijos, tipos claros, longitudes acotadas y estructuras que no cambien entre ejecuciones.

### Parte 4: M√©tricas m√≠nimas: tokens y latencia

```python
_ = (prompt | llm).invoke({"tema": "transformers vs RNNs"})
print("Traza enviada (ver LangSmith).")
```

#### Resultados
![Tabla comparativa](../assets/resultado-t14-10.png)

Vemos el Proyecto default dentro de LangSmith. Ah√≠ muestra un peque√±o resumen de la actividad, cu√°ntas ejecuciones tuviste esta semana, la tasa de errores y la latencia promedio. Se hicieron 27 runs registrados, con un 11% de errores, y una latencia normal bastante razonable P50 ‚âà 6.5s.

## Reflexi√≥n


#### ¬øQu√© prompt te cost√≥ m√°s tokens?
##### Seguramente el que ped√≠a explicaciones t√©cnicas largas, como ‚ÄúTransformers vs RNNs‚Äù, porque el modelo responde con m√°s detalle y eso consume m√°s tokens.

#### ¬øC√≥mo balancear latencia vs calidad?
##### Si se queire menos latencia, se usa prompts m√°s cortos, tambi√©n max_tokens m√°s bajo o un modelo m√°s chico. Si quer√©s m√°s calidad, dej√°s que el modelo genere m√°s texto o eleg√≠s uno m√°s grande. Es b√°sicamente ajustar velocidad vs detalle seg√∫n lo que se necesite.


### Parte 5: Mini-tareas guiadas

```python
# Esqueleto sugerido para 1) y 2)
from pydantic import BaseModel

class Traduccion(BaseModel):
    text: str
    lang: str

traductor = llm.with_structured_output(Traduccion)
salida = traductor.invoke("Traduc√≠ al portugu√©s: 'Excelente trabajo del equipo'.")
print(salida)

# Q&A con contexto (sin RAG)
from langchain_core.prompts import ChatPromptTemplate
QA_prompt = ChatPromptTemplate.from_messages([
    ("system", "Respond√© SOLO usando el contexto. Si no alcanza, dec√≠ 'No suficiente contexto'."),
    ("human",  "Contexto:\n{contexto}\n\nPregunta: {pregunta}\nRespuesta breve:")
])
salida = (QA_prompt | llm).invoke({
    "contexto": "OpenAI y LangChain permiten structured output con JSON...",
    "pregunta": "¬øQu√© ventaja tiene structured output?"
})
print(salida)
```

#### Resultados
> "text='Excelente trabalho da equipe.' lang='portugu√©s'
content='No suficiente contexto' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 268, 'prompt_tokens': 54, 'total_tokens': 322, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Cd2dZ0Dw0Otvf0wDo2HJViLksA4LD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--1220adb5-8e3c-4711-b61e-f6ad1aed78c2-0' usage_metadata={'input_tokens': 54, 'output_tokens': 268, 'total_tokens': 322, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}"

La parte de traducci√≥n anduvo bien, devolvi√≥ text='Excelente trabalho da equipe.' y lang='portugu√©s', as√≠ que el JSON estructurado funcion√≥. En la parte de Q&A, el LLM respondi√≥ "No suficiente contexto", o sea que no hab√≠a info en el bloque para contestar la pregunta.

## Reflexi√≥n

#### ¬øCu√°ndo ‚Äúalucina‚Äù el modelo al no tener suficiente contexto?
##### Cuando la pregunta pide algo que no est√° en el contexto y el prompt no es claro en que debe decir ‚ÄúNo suficiente contexto‚Äù. Si no pon√©s esa regla, el modelo intenta completar la info inventando.

#### ¬øC√≥mo exigir formato y concisi√≥n de manera consistente?
##### Definiendo el formato en el prompt, usando with_structured_output y poniendo l√≠mites claros, como temperature=0, m√°ximo de tokens y secciones fijas. Esto obliga al modelo a seguir siempre la misma estructura.


### Parte 6: Zero-shot vs Few-shot

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# Zero-shot
zs_prompt = ChatPromptTemplate.from_messages([
    ("system", "Sos un asistente conciso y exacto."),
    ("human",  "Clasific√° el sentimiento de este texto como POS, NEG o NEU:\n\n{texto}")
])

# Few-shot (1‚Äì2 ejemplos)
fs_prompt = ChatPromptTemplate.from_messages([
    ("system", "Sos un asistente conciso y exacto."),
    ("human",  "Ejemplo:\nTexto: 'El producto super√≥ mis expectativas'\nEtiqueta: POS"),
    ("human",  "Ejemplo:\nTexto: 'La entrega fue tarde y vino roto'\nEtiqueta: NEG"),
    ("human",  "Ejemplo:\nTexto: 'Me encanta este servicio'\nEtiqueta: POS"),
    ("human",  "Ejemplo:\nTexto: 'El soporte fue terrible'\nEtiqueta: NEG"),
    ("human",  "Ejemplo:\nTexto: 'Es aceptable, nada especial'\nEtiqueta: NEU"),
    ("human",  "Texto: {texto}\nEtiqueta:")
])

textos = [
    "Me encant√≥ la experiencia, repetir√≠a.",
    "No cumple lo prometido; decepcionante.",
    "Est√° bien, nada extraordinario."
]

for temp in [0.2, 0.8]:
    print(f"\n===== TEMPERATURE = {temp} =====")

    llm = ChatOpenAI(model="gpt-5-mini", temperature=temp)

    print("== Zero-shot ==")
    for t in textos:
        print((zs_prompt | llm).invoke({"texto": t}).content)

    print("\n== Few-shot ==")
    for t in textos:
        print((fs_prompt | llm).invoke({"texto": t}).content)
```

Se modific√≥ un poco el c√≥digo base para completar con las consignas. Agregu√© m√°s ejemplos para las etiquetas e hice un for para comprar las temp, y a su vez, comparar zero-shot vs few-shot.

#### Resultados
![Tabla comparativa](../assets/resultado-t14-11.png)

Con temperature 0.2, zero-shot fue completamente estable y devolvi√≥ POS / NEG / NEU, mientras que few-shot acert√≥ las etiquetas pero mostr√≥ inconsistencia en el formato (a veces con ‚ÄúEtiqueta:‚Äù, a veces sin). Con temperature 0.8, zero-shot se mantuvo igual de estable, y few-shot pas√≥ a ser m√°s consistente en el formato, devolviendo siempre ‚ÄúEtiqueta: X‚Äù.

### Parte 7: Res√∫menes: single-doc y multi-doc

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI


long_text = """Peg√° ac√° un texto largo para experimentar. Repetilo para simular longitud."""

# Split en chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
chunks = splitter.split_text(long_text)

# Cadena para resumir un chunk
chunk_summary_prompt = ChatPromptTemplate.from_messages([
    ("system", "Resum√≠ el siguiente fragmento en 2‚Äì3 bullets, claros y factuales."),
    ("human", "{input}")
])

llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
chunk_summary = chunk_summary_prompt | llm

bullets = [chunk_summary.invoke({"input": c}).content for c in chunks]

# Reduce (combinar resultados)
reduce_prompt = ChatPromptTemplate.from_messages([
    ("system", "Consolid√° bullets redundantes y produc√≠ un resumen √∫nico y breve."),
    ("human", "Bullets:\n{bullets}\n\nResumen final (<=120 tokens):")
])

final = (reduce_prompt | llm).invoke({"bullets": "\n".join(bullets)}).content
print(final)
```

#### Resultados
##### Texto base
> "Pide al usuario que pegue un texto largo para un experimento y que lo repita si quiere simular una mayor longitud."
##### Texto m√°s largo
> "Solicita pegar y repetir un texto largo para simular mayor longitud como pr√°ctica experimental destinada a evaluar c√≥mo el modelo maneja contenido extenso y, en particular, c√≥mo realiza res√∫menes cuando se supera su l√≠mite c√≥modo de contexto."
##### Texto muy largo y duplicado
> "Pide pegar y repetir un mismo texto para simular entradas muy largas; objetivo: evaluar c√≥mo el modelo resume cuando el contenido supera su l√≠mite c√≥modo de contexto; m√©todo: duplicar o triplicar el bloque para generar distintas longitudes y comparar los res√∫menes obtenidos."

Los resultados muestran que, a medida que el texto se vuelve m√°s largo y repetido, el resumen va agregando m√°s detalles sobre el prop√≥sito del experimento. El texto base produce una idea simple como ‚Äúpegar un texto largo‚Äù, el segundo testeo usa un texto m√°s largo que incluye ya el objetivo ‚Äúevaluar manejo de contenido extenso‚Äù. En el texto muy largo y duplicado, el modelo capta incluso el m√©todo ‚Äúduplicar o triplicar el bloque‚Äù y arma un resumen m√°s completo, demostrando que mayor longitud aporta m√°s se√±ales para una s√≠ntesis m√°s rica.


## Exploraci√≥n

#### Compar√° ‚Äúresumen directo‚Äù (sin split) vs map-reduce.
##### El resumen directo tiende a ser m√°s coherente porque el modelo ve todo el texto junto, pero puede perder detalles si el input es muy largo o al menos puede costar m√°s tokens. En cambio, map-reduce mantiene mejor la cobertura del contenido largo, aunque a veces genera bullets repetidos.

#### ¬øC√≥mo afectan chunk_size y chunk_overlap la calidad?
##### El chunk_size afecta el nivel de contexto, es decir, chunks chicos pierden informaci√≥n global, chunks grandes mejoran coherencia pero suben el costo. Por otro lado, el chunk_overlap ayuda a no cortar ideas a la mitad porque demasiado poco genera res√∫menes fragmentados, y demasiado, solapa contenido y produce redundancia.


### Parte 8: Extracci√≥n de informaci√≥n

```python
from typing import List, Optional
from pydantic import BaseModel
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-5-mini", temperature=0)

class Entidad(BaseModel):
    tipo: str   # p.ej., 'ORG', 'PER', 'LOC'
    valor: str

class ExtractInfo(BaseModel):
    titulo: Optional[str]
    fecha: Optional[str]
    entidades: List[Entidad]

extractor = llm.with_structured_output(ExtractInfo)
pruebas = [
    "Ayer OpenAI present√≥ un informe conjunto con UCU.",
    "Se firm√≥ un acuerdo el jueves pasado en Montevideo.",
    "La empresa XCorp colabora con JP y AC, en un proyecto sin fecha definida.",
    "Colaboraci√≥n entre OpenAI y la Universidad Cat√≥lica del Uruguay el 5-11."
]

for t in pruebas:
    print("\nTEXTO:", t)
    print(extractor.invoke(f"Extra√© titulo, fecha y entidades:\n{t}"))

```

#### Resultados
##### Texto
> "titulo=None fecha='2025-11-17' entidades=[Entidad(tipo='organizaci√≥n', valor='OpenAI'), Entidad(tipo='organizaci√≥n', valor='UCU')]"
##### Texto2
> "titulo=None fecha='jueves pasado' entidades=[Entidad(tipo='Fecha', valor='jueves pasado'), Entidad(tipo='Lugar', valor='Montevideo'), Entidad(tipo='Evento', valor='acuerdo')]"
##### Texto3
> "titulo=None fecha=None entidades=[Entidad(tipo='empresa', valor='XCorp'), Entidad(tipo='persona', valor='JP'), Entidad(tipo='persona', valor='AC')]"
##### Texto4
> "titulo='Colaboraci√≥n entre OpenAI y la Universidad Cat√≥lica del Uruguay' fecha='5-11' entidades=[Entidad(tipo='organizaci√≥n', valor='OpenAI'), Entidad(tipo='organizaci√≥n', valor='Universidad Cat√≥lica del Uruguay')]"

Los resultados muestran que el modelo reconoce bien entidades conocidas, pero es inconstante con los tipos, mezcla ORG/Fecha/Lugar/Evento y a veces alucina datos, como la fecha inventada ‚Äú2025-11-17‚Äù. Las fechas ambiguas como ‚Äújueves pasado‚Äù no las normaliza y las iniciales ‚ÄúJP‚Äù, ‚ÄúAC‚Äù las interpreta como personas sin mucha seguridad. Cuando el texto est√° m√°s estructurado, extrae mejor t√≠tulo y fecha, pero aun as√≠ no estandariza formatos, lo que muestra l√≠mites en robustez y consistencia.

### Parte 9: RAG b√°sico con textos locales

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS  # Si us√°s Chroma, import√° su VectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain_classic.chains import create_retrieval_chain
from langchain_core.documents import Document

docs_raw = [
    "LangChain soporta structured output con Pydantic.",
    "RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.",
    "OpenAIEmbeddings facilita embeddings para indexar textos."
]
docs = [Document(page_content=t) for t in docs_raw]

# Split y vector store
splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = splitter.split_documents(docs)

emb = OpenAIEmbeddings()
vs = FAISS.from_documents(chunks, embedding=emb)

llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
prompt = ChatPromptTemplate.from_messages([
    ("system", "Respond√© SOLO con el contexto. Si no alcanza, dec√≠ 'No suficiente contexto'."),
    ("human",  "Contexto:\n{context}\n\nPregunta: {input}")
])

combine_docs_chain = create_stuff_documents_chain(llm, prompt)

for k in [1, 2, 4]:
    print(f"\n  k = {k} ")
    
    retriever = vs.as_retriever(search_kwargs={"k": k})
    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)

    out = rag_chain.invoke({"input": "¬øQu√© ventaja clave aporta RAG?"})
    print(out)
```

Se cambi√≥ un poco el c√≥digo base para probar con distintos K¬¥s.

#### Resultados
##### K=1
>"{'input': '¬øQu√© ventaja clave aporta RAG?', 'context': [Document(id='17da820f-4db3-4d5f-901d-2d3cd222283f', metadata={}, page_content='RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.')], 'answer': 'RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.'}"
##### K=2
>"{'input': '¬øQu√© ventaja clave aporta RAG?', 'context': [Document(id='17da820f-4db3-4d5f-901d-2d3cd222283f', metadata={}, page_content='RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.'), Document(id='202b3eba-b80a-48e6-9fc3-f4cbb37daade', metadata={}, page_content='OpenAIEmbeddings facilita embeddings para indexar textos.')], 'answer': 'RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.'}"
##### K=4
>"{'input': '¬øQu√© ventaja clave aporta RAG?', 'context': [Document(id='17da820f-4db3-4d5f-901d-2d3cd222283f', metadata={}, page_content='RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.'), Document(id='202b3eba-b80a-48e6-9fc3-f4cbb37daade', metadata={}, page_content='OpenAIEmbeddings facilita embeddings para indexar textos.'), Document(id='b59f8207-1e78-4967-a68a-6c227b348b0f', metadata={}, page_content='LangChain soporta structured output con Pydantic.')], 'answer': 'RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.'}"

En estos resultados, aumentar K no cambia la respuesta porque toda la informaci√≥n relevante est√° concentrada en un √∫nico chunk, por lo que K=1 recupera exactamente lo necesario. Al subir a K=2 y K=4, el sistema agrega chunks un poco irrelevantes, y es lo entendemos como ruido pero no afecta la salida porque el prompt obliga a usar solo el contexto relevante.
